{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10723917",
   "metadata": {},
   "source": [
    "# From Erasure to Transplant: Probing Embedding Semantics\n",
    "## Stress-Testing of Text-to-Image Models via Semantic Surgery\n",
    "\n",
    "**Course:** Advanced Machine Learning and Computer Vision (2025)\n",
    "\n",
    "**Framework:** Based on *Semantic Surgery* (NeurIPS 2025)\n",
    "\n",
    "**Students:** Lorenzo Musso (*2049518*) - Giulia Pietrangeli (*2057291*)\n",
    "\n",
    "---\n",
    "\n",
    "### Abstract\n",
    "This project proposes a paradigm shift in the analysis of Text-to-Image (T2I) models. Moving beyond standard safety-oriented concept erasure, we repurpose the **Semantic Surgery** framework as a diagnostic probe to perform **Semantic Transplantation**. By injecting semantic shift vectors with **Token-Wise Precision**, we stress-test **Stable Diffusion v1.4** to measure its robustness, interpretability, and latent semantic entanglement. Crucially, we introduce the **'Surgery Autopilot'**—comparing **Classical Machine Learning** and **Deep Learning** approaches—to **automate the optimization of surgical hyperparameters**, effectively bridging diagnostic analysis with practical control.\n",
    "\n",
    "### Experimental Framework\n",
    "\n",
    "1.  **Methodology:** We extend the original vector subtraction mechanism to **Vector Injection**:\n",
    "    $$c^* = c_{in} + \\lambda \\cdot \\mathbf{M}_{\\alpha} \\cdot (v_{new} - v_{old})$$\n",
    "    Where $\\lambda$ represents the intervention **Force** and $\\mathbf{M}_{\\alpha}$ is the **Sensitivity Mask**.\n",
    "\n",
    "2. **Investigation I (Validation):** Object & Context Swapping (with Manual Fine-Tuning) to evaluate **Semantic Alignment** and **Multi-Scale Fidelity**:\n",
    "   * **Locality & Pose Preservation** (Owl-ViT & IoU)\n",
    "   * **Semantic Integrity** (CLIP & ResNet50)\n",
    "   * **Structural Fidelity** (SSIM & LPIPS)\n",
    "   * **Spectral Analysis** (FFT)\n",
    "\n",
    "3.  **Investigation II (Ablation):** Systematic Hyperparameter Grid Search to empirically identify the **Optimal Trade-off** between editing efficacy ($\\lambda$) and background preservation ($\\alpha$).\n",
    "\n",
    "4. **Investigation III (Stress-Test):** Adversarial generation on a **Custom Benchmark Dataset** (configured with manually selected parameters) to quantify latent model failures:\n",
    "   * **Contextual Bias:** Testing Robustness against OOD environments (e.g., *Boat in Desert*).\n",
    "   * **Attribute Entanglement:** Measuring Visual Leakage (Colour/Texture) between concepts.\n",
    "   * **Societal Bias:** Analyzing implicit Gender Shifts in occupational swaps.\n",
    "\n",
    "5. **Investigation IV (Automation Phase):** The \"Surgery Autopilot\".\n",
    "      * **Data Generation:** An exhaustive Grid Search creates a ground-truth dataset mapping prompts to optimal parameters.\n",
    "      * **Feature Extraction:** Prompts are transformed into high-dimensional **CLIP Embeddings**.\n",
    "      * **Model Competition:** We train and compare two architectures:\n",
    "        * **Baseline:** Multi-Output Random Forest (ML).\n",
    "        * **SurgeryNet:** A custom Multi-Layer Perceptron (DL) with regularization.\n",
    "      * **Evaluation:** Head-to-head comparison using **MAE** to determine the superior approach.\n",
    "\n",
    "6. **Live Demonstration:** An interactive **Gradio Interface** allowing real-time comparison between **Manual Control**, **ML Prediction**, and **DL Prediction**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129ef766",
   "metadata": {},
   "source": [
    "# 1. Environment Setup & Initialization\n",
    "\n",
    "This section establishes the technical foundation for the project. It performs four critical tasks to prepare the environment for *Semantic Surgery*:\n",
    "\n",
    "1.  **Dependency Management**: Loads essential libraries (`torch`, `numpy`, `PIL`) and specific evaluation architectures used throughout the pipeline (**ResNet50** for fidelity, **LPIPS/SSIM** for structural similarity, and **CLIP** for semantic alignment).\n",
    "2.  **Reproducibility & Hardware Acceleration**:\n",
    "    * `seed_everything(42)`: Fixes the random seed across PyTorch, NumPy, and Python to ensure deterministic diffusion generation.\n",
    "    * **Device Selection**: Automatically detects hardware acceleration, prioritizing Apple Silicon (`mps`) or NVIDIA GPUs (`cuda`) to optimize inference speed.\n",
    "3.  **Model Loading (The \"Surgeon\")**:\n",
    "    * Initializes the `Evaluator` class for real-time metrics.\n",
    "    * **StableDiffuser**: Loads the core diffusion model with the **DDIM Scheduler**.\n",
    "    * **Default Hyperparameters**:\n",
    "        * `beta (-0.12)`: **Sensitivity** (Controls attention mask strictness).\n",
    "        * `lambda (1.0)`: **Force** (Controls semantic vector injection intensity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0956abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Seed set to: 42\n",
      "Hardware: Apple Silicon detected! Using device: mps (GPU)\n",
      "Working Directory: /Users/giulia/Documents/Università/Advanced Machine Learning/Final Project/Semantic Surgery\n",
      "Local 'src' modules imported successfully.\n",
      "Initializing Evaluator (Lazy Loading Mode) on mps...\n",
      "Initialising StableDiffuser Model...\n",
      "Loading VAE...\n",
      "Loading tokenizer and text encoder...\n",
      "Loading UNet model...\n",
      "Loading feature extractor and safety checker...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/surgery/lib/python3.10/site-packages/transformers/models/clip/feature_extraction_clip.py:30: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up scheduler...\n",
      "All components loaded successfully.\n",
      "Model successfully loaded on mps.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# --- 1. SYSTEM & UTILITIES IMPORTS ---\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import gc\n",
    "import random\n",
    "import pickle\n",
    "import itertools\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# --- 2. DATA SCIENCE & VIZ IMPORTS ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import cv2\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# --- 3. PYTORCH & METRICS IMPORTS ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import lpips\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "# --- 4. MODEL ARCHITECTURES IMPORTS ---\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "import torchvision.transforms as transforms\n",
    "from transformers import (\n",
    "    ViTForImageClassification, ViTImageProcessor,\n",
    "    BlipProcessor, BlipForConditionalGeneration,\n",
    "    CLIPProcessor, CLIPModel,\n",
    "    OwlViTProcessor, OwlViTForObjectDetection,\n",
    "    CLIPSegProcessor, CLIPSegForImageSegmentation\n",
    ")\n",
    "\n",
    "# --- 5. SKLEARN & GRADIO IMPORTS ---\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import gradio as gr\n",
    "\n",
    "# --- REPRODUCIBILITY SETUP ---\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    print(f\"Global Seed set to: {seed}\")\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "# --- HARDWARE DETECTION ---\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "    print(f\"Hardware: Apple Silicon detected! Using device: {device} (GPU)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    print(f\"Hardware: NVIDIA GPU detected! Using device: {device}\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"Hardware: Warning! Using CPU (Inference will be slow).\")\n",
    "\n",
    "current_dir = os.path.abspath(\".\")\n",
    "if current_dir not in sys.path:\n",
    "    sys.path.append(current_dir)\n",
    "    \n",
    "print(f\"Working Directory: {current_dir}\")\n",
    "print(\"Local 'src' modules imported successfully.\")\n",
    "\n",
    "# --- MODULE IMPORTS ---\n",
    "import src.utils as utils\n",
    "import src.evaluation as evaluation\n",
    "\n",
    "# --- MODEL INITIALIZATION ---\n",
    "if 'evaluator' not in locals():\n",
    "    evaluator = evaluation.Evaluator(device)\n",
    "\n",
    "print(f\"Initialising StableDiffuser Model...\")\n",
    "\n",
    "init_params = {\n",
    "    \"gamma\": 0.02, \n",
    "    \"beta\": -0.12,     # Base Sensitivity \n",
    "    \"lambda\": 1.0,     # Base Force \n",
    "    \"alpha_threshold\": 0.2, \n",
    "    \"detect_threshold\": 0.5,\n",
    "    \"alpha_f\": None, \n",
    "    \"erase_index_f\": None\n",
    "}\n",
    "\n",
    "try:\n",
    "    surgeon = utils.StableDiffuser(\n",
    "        scheduler=\"DDIM\",\n",
    "        cache_dir=\"./model_cache\", \n",
    "        concepts_to_erase=[\"placeholder\"], \n",
    "        params=init_params\n",
    "    )\n",
    "    surgeon = surgeon.to(device) \n",
    "    print(f\"Model successfully loaded on {device}.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Initialisation Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b406ba99",
   "metadata": {},
   "source": [
    "# 2. Investigation I: Method Validation (Proof of Concept)\n",
    "\n",
    "## Part A: Object Swapping Pipeline\n",
    "\n",
    "This section executes the **Data Production Phase** for the primary validation task: Object Swapping. The goal is to generate a visual dataset (Original vs. Swapped) that will be subjected to quantitative analysis to verify if the method can successfully replace an object while preserving the surrounding context.\n",
    "\n",
    "### Methodology\n",
    "1.  **Scenario Definition**: We define a set of test cases ranging from natural subjects (*Bear → Tiger*) to inanimate objects (*Sportscar → Firetruck*).\n",
    "2.  **Hyperparameter Selection**: Each scenario uses **custom hyperparameters** (`Force`, `Sensitivity`) derived from preliminary tuning to ensure optimal generation quality for the validation step.\n",
    "3.  **Deterministic Generation**: The pipeline runs with a fixed seed (`42`) to guarantee that the \"Original\" and \"Modified\" images share the exact same initial noise latent, ensuring that any visual difference is solely due to the semantic intervention.\n",
    "\n",
    "### Outputs\n",
    "* **Visual Dataset**: Images are saved to `results/subject_swap_results/`.\n",
    "* **Preliminary Inspection**: A 3x2 grid is generated for each case, displaying the visual result alongside **Grad-CAM Attention Maps** to verify if the model's focus has correctly shifted to the new semantic concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d930e69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All validation scenarios completed.\n"
     ]
    }
   ],
   "source": [
    "base_dir = \"results/subject_swap_results\"\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "print(f\"Saving validation results to: {base_dir}/\")\n",
    "\n",
    "# Define Validation Scenarios\n",
    "swap_scenarios = [\n",
    "    # ANIMALS (Texture & Shape changes)\n",
    "    {\"name\": \"Swap 1: Forest\", \"base_prompt\": \"A brown bear walking in a forest\", \"old_obj\": \"bear\", \"new_obj\": \"tiger\", \"remove\": \"brown bear\", \"replace\": \"tiger\", \"force\": 1.3, \"sens\": 0.15},\n",
    "    {\"name\": \"Swap 2: Sofa\", \"base_prompt\": \"A dog sitting on a sofa\", \"old_obj\": \"dog\", \"new_obj\": \"cat\", \"remove\": \"dog\", \"replace\": \"cat\", \"force\": 1.0, \"sens\": 0.2},\n",
    "    {\"name\": \"Swap 3: Fish\", \"base_prompt\": \"A goldfish swimming in the sea\", \"old_obj\": \"goldfish\", \"new_obj\": \"shark\", \"remove\": \"goldfish\", \"replace\": \"shark\", \"force\": 1.0, \"sens\": 0.2},\n",
    "    # OBJECTS (Rigid structures)\n",
    "    {\"name\": \"Swap 4: Street\", \"base_prompt\": \"A red sportscar driving on a asphalt road\", \"old_obj\": \"sportscar\", \"new_obj\": \"firetruck\", \"remove\": \"red sportscar\", \"replace\": \"firetruck\", \"force\": 0.8, \"sens\": 0.25},\n",
    "    {\"name\": \"Swap 5: Office\", \"base_prompt\": \"A coffee mug on a wooden office desk\", \"old_obj\": \"mug\", \"new_obj\": \"beer\", \"remove\": \"coffee mug\", \"replace\": \"beer bottle\", \"force\": 1.0, \"sens\": 0.3},\n",
    "    {\"name\": \"Swap 6: Table\", \"base_prompt\": \"A red apple on a wooden table\", \"old_obj\": \"apple\", \"new_obj\": \"daisy\", \"remove\": \"red apple\", \"replace\": \"daisy flower\", \"force\": 1.0, \"sens\": 0.3}\n",
    "]\n",
    "\n",
    "json_swap_list = []\n",
    "\n",
    "if 'surgeon' in locals():\n",
    "    print(\"Starting Object Swap Validation...\")\n",
    "    surgeon = surgeon.to(device)\n",
    "    \n",
    "    for i, scen in enumerate(swap_scenarios):\n",
    "        gc.collect()\n",
    "        if torch.backends.mps.is_available(): torch.mps.empty_cache()\n",
    "        \n",
    "        print(f\"\\nTesting Scenario [{i+1}/{len(swap_scenarios)}]: {scen['name']}\")\n",
    "        \n",
    "        # Apply Scenario Params\n",
    "        surgeon.params['lambda'] = scen['force']\n",
    "        surgeon.params['alpha_threshold'] = scen['sens']\n",
    "        \n",
    "        safe_name = scen['name'].replace(\" \", \"_\").replace(\":\", \"\")\n",
    "        scenario_dir = os.path.join(base_dir, safe_name)\n",
    "        os.makedirs(scenario_dir, exist_ok=True)\n",
    "        file_paths = {}\n",
    "        \n",
    "        full_prompt = f\"Photography, 8k, {scen['base_prompt']}\"\n",
    "        \n",
    "        def process_step(variant_name, replace_txt, erase_txt):\n",
    "            gen_step = torch.Generator(\"cpu\").manual_seed(42)\n",
    "            \n",
    "            surgeon.concepts_to_erase = [erase_txt] if erase_txt else []\n",
    "            \n",
    "            print(f\"   > Generating {variant_name}...\")\n",
    "            imgs = surgeon([full_prompt], img_size=512, n_steps=30, n_imgs=1, \n",
    "                           show_alpha=False, generator=gen_step, replace_with=replace_txt)\n",
    "            img = imgs[0][0]\n",
    "            \n",
    "            path = os.path.join(scenario_dir, f\"{variant_name}.png\")\n",
    "            img.save(path)\n",
    "            file_paths[variant_name] = path\n",
    "\n",
    "            # Compute Quick Metrics (Top-2 Class + GradCAM)\n",
    "            top2 = evaluator.get_top2_verdict(img)\n",
    "            cam1 = evaluator.compute_gradcam(img, top2[0]['id']) \n",
    "            cam2 = evaluator.compute_gradcam(img, top2[1]['id']) \n",
    "            \n",
    "            return {\"img\": img, \"top2\": top2, \"cam1\": cam1, \"cam2\": cam2, \"title\": variant_name}\n",
    "    \n",
    "        # Execute Steps\n",
    "        res_orig = process_step(\"Original\", None, None)\n",
    "        res_swap = process_step(\"Swap\", scen['replace'], scen['remove'])\n",
    "        \n",
    "        # Log Data\n",
    "        json_swap_list.append({\n",
    "            \"scenario\": scen['name'],\n",
    "            \"old_obj\": scen['old_obj'], \"new_obj\": scen['new_obj'],\n",
    "            \"paths\": file_paths,\n",
    "            \"metrics\": {\n",
    "                \"orig_top1\": res_orig['top2'][0],\n",
    "                \"swap_top1\": res_swap['top2'][0]\n",
    "            }\n",
    "        })\n",
    "        \n",
    "        # --- VISUALIZATION ---\n",
    "        fig, axes = plt.subplots(3, 2, figsize=(10, 14))\n",
    "        fig.suptitle(f\"{scen['name']}: {scen['old_obj']} ➝ {scen['new_obj']}\", fontsize=16, fontweight='bold', y=0.99)\n",
    "        \n",
    "        columns = [res_orig, res_swap]\n",
    "        for col_idx, data in enumerate(columns):\n",
    "            # Row 1: Image + Classification\n",
    "            ax_img = axes[0, col_idx]\n",
    "            ax_img.imshow(data['img'])\n",
    "            verdict = f\"1. {data['top2'][0]['name']} ({data['top2'][0]['score']:.1%})\\n2. {data['top2'][1]['name']} ({data['top2'][1]['score']:.1%})\"\n",
    "            ax_img.set_title(f\"{data['title']}\\n{verdict}\", fontsize=10, loc='left', \n",
    "                             bbox=dict(facecolor='white', alpha=0.8, edgecolor='gray', boxstyle='round'))\n",
    "            ax_img.axis(\"off\")\n",
    "            \n",
    "            # Row 2: GradCAM 1st Prediction\n",
    "            ax_cam1 = axes[1, col_idx]\n",
    "            ax_cam1.imshow(data['cam1'])\n",
    "            ax_cam1.set_title(f\"Focus: {data['top2'][0]['name']}\", fontsize=10, color='blue', fontweight='bold')\n",
    "            ax_cam1.axis(\"off\")\n",
    "\n",
    "            # Row 3: GradCAM 2nd Prediction\n",
    "            ax_cam2 = axes[2, col_idx]\n",
    "            ax_cam2.imshow(data['cam2'])\n",
    "            ax_cam2.set_title(f\"Focus: {data['top2'][1]['name']}\", fontsize=10, color='red')\n",
    "            ax_cam2.axis(\"off\")\n",
    "            \n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "        plt.savefig(os.path.join(base_dir, f\"Analysis_{safe_name}.png\"), dpi=100, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        plt.close(fig) \n",
    "        del res_orig, res_swap, fig, axes \n",
    "    \n",
    "    # Save Metadata for next steps\n",
    "    with open(os.path.join(base_dir, \"validation_metrics.json\"), 'w') as f:\n",
    "        json.dump(json_swap_list, f, indent=4)\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    print(\"\\nAll validation scenarios completed.\")\n",
    "else:\n",
    "    print(\"Error: Surgeon model not loaded. Run Setup cell first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4180aa6",
   "metadata": {},
   "source": [
    "### Spatial Consistency Analysis (Locality & Pose)\n",
    "\n",
    "This section performs a quantitative validation of the **Locality** and **Pose Preservation** properties of the Semantic Surgery. The objective is to verify that the semantic intervention remains strictly confined to the target object's bounding box, answering the question: *\"Did the new object appear in the exact same physical location as the old one?\"*\n",
    "\n",
    "#### Methodology\n",
    "1.  **Automated Object Detection (Owl-ViT)**:\n",
    "    * We employ an Open-Vocabulary Object Detector (**Owl-ViT**) to blindly localize the target concepts in both the *Original* and *Modified* images.\n",
    "    * This allows for an unbiased extraction of bounding boxes (e.g., detecting the \"Bear\" in image A and the \"Tiger\" in image B) without human annotation.\n",
    "\n",
    "2.  **IoU Calculation (Intersection over Union)**:\n",
    "    * We calculate the **IoU Score** between the bounding box of the original object ($B_{old}$) and the new object ($B_{new}$).\n",
    "    * **Metric Interpretation**:\n",
    "        * **High IoU ($\\approx 1.0$)**: Excellent Pose Preservation. The new object perfectly occupies the spatial footprint of the old one.\n",
    "        * **Low IoU ($\\approx 0.0$)**: Poor Locality. The object has shifted significantly or the model generated the new concept in a different area.\n",
    "\n",
    "3.  **Visualization**:\n",
    "    * **Qualitative**: A 3x2 grid visualizing the overlap of bounding boxes (Red = Old, Green = New).\n",
    "    * **Quantitative**: A bar chart plotting the IoU scores against a \"Stability Threshold\" ($0.5$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdf1af12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Owl-ViT Analysis Completed.\n"
     ]
    }
   ],
   "source": [
    "base_dir = \"results/subject_swap_results\"\n",
    "json_path = os.path.join(base_dir, \"validation_metrics.json\")\n",
    "print(f\"Analysis IoU box in: {base_dir}/\")\n",
    "\n",
    "if os.path.exists(json_path):\n",
    "    with open(json_path, 'r') as f: experiments = json.load(f)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 2, figsize=(12, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    print(\"   Detecting Objects...\")\n",
    "    \n",
    "    for idx, exp in enumerate(experiments):\n",
    "        if idx >= 6: break\n",
    "        \n",
    "        name = exp['scenario'].split(':')[1].strip()\n",
    "        p_orig = exp['paths']['Original']\n",
    "        p_swap = exp['paths']['Swap']\n",
    "        \n",
    "        if os.path.exists(p_orig) and os.path.exists(p_swap):\n",
    "            im_o = Image.open(p_orig).convert('RGB')\n",
    "            im_s = Image.open(p_swap).convert('RGB')\n",
    "            \n",
    "            # 1. Detect Bounding Boxes\n",
    "            box_old = evaluator.get_greedy_box(im_o, exp['old_obj'])\n",
    "            box_new = evaluator.get_greedy_box(im_s, exp['new_obj'])\n",
    "            \n",
    "            # 2. Compute IoU\n",
    "            iou = 0.0\n",
    "            if box_old is not None and box_new is not None:\n",
    "                iou = evaluator.compute_iou(box_old, box_new)\n",
    "            \n",
    "            results.append({\"Scenario\": name, \"IoU\": iou})\n",
    "            \n",
    "            # 3. Visualize Overlap\n",
    "            combined_img = evaluator.draw_box_comparison(im_o, box_old, im_s, box_new)\n",
    "            \n",
    "            ax = axes[idx]\n",
    "            ax.imshow(combined_img)\n",
    "            ax.set_title(f\"{name}\\nIoU: {iou:.2f}\", fontsize=11, fontweight='bold')\n",
    "            ax.axis('off')\n",
    "            \n",
    "            # Save individual check\n",
    "            combined_img.save(os.path.join(base_dir, f\"BoxOverlap_{name}.png\"))\n",
    "\n",
    "    # Finalize Grid Plot\n",
    "    plt.suptitle(\"Spatial Consistency (Multi-Object Coverage)\", y=0.99, fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(base_dir, \"ObjectSwap_Box_Grid.png\"), dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    # Generate Quantitative Bar Chart\n",
    "    if results:\n",
    "        df = pd.DataFrame(results)\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        sns.barplot(data=df, x=\"Scenario\", y=\"IoU\", palette=\"viridis\")\n",
    "        plt.title(\"Spatial Precision Score\")\n",
    "        plt.ylabel(\"IoU Score\")\n",
    "        plt.ylim(0, 1.0)\n",
    "        plt.axhline(0.5, color='red', ls='--', label=\"Stability Threshold\")\n",
    "        plt.grid(axis='y', alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(base_dir, \"ObjectSwap_BoxIoU_Chart.png\"), dpi=150)\n",
    "        plt.show()\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    print(\"Owl-ViT Analysis Completed.\")\n",
    "else:\n",
    "    print(\"JSON file not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5391b9",
   "metadata": {},
   "source": [
    "### Quantitative Analysis – Multi-Scale Fidelity (Object Swap)\n",
    "\n",
    "This cell executes the comprehensive evaluation of the Object Swapping experiment. It aggregates data from the generated images and subjects them to a battery of **5 distinct analytical tests** to validate the *Semantic Surgery* performance across different scales of perception.\n",
    "\n",
    "#### Key Analytical Modules:\n",
    "\n",
    "1.  **Semantic Integrity Analysis (The \"Meaning\" Check)**:\n",
    "    * **ResNet-50 Confidence**: Evaluates the classification probability for both the *Old Object* (Ghost Score) and the *New Object* (Success Score).\n",
    "        * *Goal:* High Success Score ($\\approx 1.0$), Low Ghost Score ($\\approx 0.0$).\n",
    "    * **CLIP Score**: Measures the semantic alignment between the visual content and the text prompts (e.g., alignment with \"a photo of a tiger\" vs \"a photo of a bear\").\n",
    "\n",
    "2.  **Structural Fidelity Analysis (The \"Quality\" Check)**:\n",
    "    * **SSIM (Structural Similarity)**: Mathematically compares pixel structure, luminance, and contrast. (*Higher is better*).\n",
    "    * **LPIPS (Perceptual Similarity)**: Uses a deep neural network to mimic human vision and detect if the image \"feels\" different. (*Lower is better*).\n",
    "\n",
    "3.  **Spectral Analysis (FFT)**:\n",
    "    * Performs **Fast Fourier Transform** to visualize the frequency domain.\n",
    "    * Calculates the **Spectral Energy Difference** to ensure the editing process didn't introduce high-frequency artifacts (noise/checkerboard patterns) invisible to the naked eye.\n",
    "\n",
    "4.  **Automated Captioning (BLIP)**:\n",
    "    * Uses the **BLIP** model to generate a neutral, AI-written caption of the final result. This serves as a \"blind test\" to verify if an external AI naturally describes the new object without being prompted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81f70eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analysis completed, images saved at: results/subject_swap_results\n",
      "   Memory Cleared.\n"
     ]
    }
   ],
   "source": [
    "base_dir = \"results/subject_swap_results\"\n",
    "swap_json = os.path.join(base_dir, \"validation_metrics.json\") \n",
    "print(f\"Starting Quantitative Analysis in: {base_dir}\")\n",
    "\n",
    "if os.path.exists(swap_json):\n",
    "    with open(swap_json, 'r') as f: experiments = json.load(f)\n",
    "    \n",
    "    swap_rows = []\n",
    "    print(\"   Processing metrics (ResNet, CLIP, SSIM, LPIPS)...\")\n",
    "\n",
    "    for exp in experiments:\n",
    "        name = exp['scenario'].split(':')[1].strip()\n",
    "        old_obj = exp['old_obj']\n",
    "        new_obj = exp['new_obj']\n",
    "        \n",
    "        path_orig = exp['paths']['Original']\n",
    "        path_swap = exp['paths']['Swap']\n",
    "            \n",
    "        if os.path.exists(path_swap) and os.path.exists(path_orig):\n",
    "            # 1. Structural Metrics (SSIM, LPIPS)\n",
    "            s, m, l = evaluator.get_structural_metrics(path_orig, path_swap)\n",
    "\n",
    "            img_swap_pil = Image.open(path_swap).convert('RGB')\n",
    "            \n",
    "            # 2. Semantic Integrity (ResNet & CLIP)\n",
    "            ghost_score = evaluator.get_resnet_conf(img_swap_pil, old_obj)\n",
    "            success_score = evaluator.get_resnet_conf(img_swap_pil, new_obj)\n",
    "            \n",
    "            clip_old = evaluator.get_clip_score(img_swap_pil, f\"a photo of a {old_obj}\")\n",
    "            clip_new = evaluator.get_clip_score(img_swap_pil, f\"a photo of a {new_obj}\")\n",
    "            \n",
    "            caption = evaluator.get_blip_caption(img_swap_pil)\n",
    "            \n",
    "            swap_rows.append({\n",
    "                \"Scenario\": name, \n",
    "                \"Ghost (ResNet)\": ghost_score, \n",
    "                \"Success (ResNet)\": success_score,\n",
    "                \"CLIP Old\": clip_old,\n",
    "                \"CLIP New\": clip_new,\n",
    "                \"SSIM\": s, \n",
    "                \"MSE\": m, \n",
    "                \"LPIPS\": l, \n",
    "                \"Caption\": caption,\n",
    "                \"Path_Orig\": path_orig, \n",
    "                \"Path_Swap\": path_swap\n",
    "            })\n",
    "            print(f\"   {name}: ResNet={success_score:.1%} | CLIP={clip_new:.1f} | SSIM={s:.2f}\")\n",
    "\n",
    "    if swap_rows:\n",
    "        df_swap = pd.DataFrame(swap_rows)\n",
    "        \n",
    "        # --- CHART 1: SEMANTIC INTEGRITY (ResNet + CLIP) ---\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n",
    "        \n",
    "        x = np.arange(len(df_swap))\n",
    "        w = 0.35\n",
    "        \n",
    "        # Subplot 1: ResNet \n",
    "        ax1.bar(x - w/2, df_swap['Ghost (ResNet)'], w, label='Old Object (Ghost)', color='#d62728', alpha=0.8)\n",
    "        ax1.bar(x + w/2, df_swap['Success (ResNet)'], w, label='New Object (Success)', color='#2ca02c', alpha=0.8)\n",
    "        ax1.set_xticks(x)\n",
    "        ax1.set_xticklabels(df_swap['Scenario'], rotation=15)\n",
    "        ax1.set_title('ResNet-50 Confidence (Is the object recognized?)')\n",
    "        ax1.set_ylabel('Confidence Probability')\n",
    "        ax1.legend()\n",
    "        ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "        # Subplot 2: CLIP\n",
    "        ax2.bar(x - w/2, df_swap['CLIP Old'], w, label='Align to Old Text', color='#ff7f0e', alpha=0.8)\n",
    "        ax2.bar(x + w/2, df_swap['CLIP New'], w, label='Align to New Text', color='#1f77b4', alpha=0.8)\n",
    "        ax2.set_xticks(x)\n",
    "        ax2.set_xticklabels(df_swap['Scenario'], rotation=15)\n",
    "        ax2.set_title('CLIP Semantic Score (Is the concept correct?)')\n",
    "        ax2.set_ylabel('CLIP Logits')\n",
    "        ax2.legend()\n",
    "        ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "        plt.suptitle(\"Semantic Integrity Analysis\", fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(base_dir, \"Semantic_Integrity_ResNet_CLIP.png\"), dpi=150)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "        # --- CHART 2: STRUCTURAL FIDELITY (SSIM + LPIPS) ---\n",
    "        fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "        \n",
    "        # Bar Plot SSIM \n",
    "        ax1.bar(df_swap['Scenario'], df_swap['SSIM'], color='purple', alpha=0.6, label='SSIM (Structure)')\n",
    "        ax1.set_ylabel('SSIM (Higher is Better)', color='purple', fontweight='bold')\n",
    "        ax1.set_ylim(0, 1.0)\n",
    "        \n",
    "        # Line Plot LPIPS \n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.plot(df_swap['Scenario'], df_swap['LPIPS'], color='orange', marker='o', lw=3, label='LPIPS (Perceptual Diff)')\n",
    "        ax2.set_ylabel('LPIPS (Lower is Better)', color='orange', fontweight='bold')\n",
    "        ax2.set_ylim(0, 0.8) \n",
    "        \n",
    "        plt.title(\"Structural Fidelity Analysis: SSIM vs LPIPS\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(base_dir, \"Structural_Fidelity_Metrics.png\"), dpi=150)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "        # --- CHART 3: FFT ANALYSIS ---\n",
    "        fig, axes = plt.subplots(len(df_swap), 3, figsize=(12, 3.5 * len(df_swap)))\n",
    "        if len(df_swap) == 1: axes = np.array([axes]) \n",
    "        \n",
    "        for idx, row in df_swap.iterrows():\n",
    "            s_orig = evaluator.get_fft_spectrum(row['Path_Orig'])\n",
    "            s_swap = evaluator.get_fft_spectrum(row['Path_Swap'])\n",
    "            \n",
    "            if s_orig is not None and s_swap is not None:\n",
    "                diff = np.abs(s_swap - s_orig)\n",
    "                energy = np.mean(diff)\n",
    "                \n",
    "                axes[idx, 0].imshow(s_orig, cmap='inferno'); axes[idx, 0].axis('off')\n",
    "                axes[idx, 0].set_title(f\"{row['Scenario']} Orig\")\n",
    "                \n",
    "                axes[idx, 1].imshow(s_swap, cmap='inferno'); axes[idx, 1].axis('off')\n",
    "                axes[idx, 1].set_title(f\"{row['Scenario']} Swap\")\n",
    "                \n",
    "                axes[idx, 2].imshow(diff, cmap='gray'); axes[idx, 2].axis('off')\n",
    "                axes[idx, 2].set_title(f\"Spectral Diff (Energy: {energy:.2f})\")\n",
    "                \n",
    "        plt.suptitle(\"Structural Fidelity: FFT Spectral Analysis\", y=1.01, fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(base_dir, \"Structural_Fidelity_FFT.png\"), dpi=150)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "        # --- BLIP REPORT ---\n",
    "        print(\"\\nBLIP CAPTIONS REPORT:\")\n",
    "        print(\"-\" * 60)\n",
    "        with open(os.path.join(base_dir, \"swap_final_report.txt\"), \"w\") as f:\n",
    "            f.write(f\"{'SCENARIO':<20} | {'CLIP NEW':<10} | {'SSIM':<6} | DESCRIPTION\\n\" + \"-\"*80 + \"\\n\")\n",
    "            for _, row in df_swap.iterrows():\n",
    "                line_scr = f\"{row['Scenario']:<20} | {row['CLIP New']:.2f}       | {row['SSIM']:.2f}   | {row['Caption']}\"\n",
    "                print(line_scr)\n",
    "                f.write(line_scr + \"\\n\")\n",
    "\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        print(f\"\\nAnalysis completed, images saved at: {base_dir}\")\n",
    "    \n",
    "    evaluator.free_memory()\n",
    "else:\n",
    "    print(\"No valid data found for the analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c66e90",
   "metadata": {},
   "source": [
    "## Part B: Contextual Swapping Pipeline (Validation)\n",
    "\n",
    "This section extends the validation scope from objects to **Contextual Environments**.\n",
    "Unlike the previous step, here we fix the **Main Subject** (e.g., a Bear) and surgically alter the **Background** to verify the method's ability to disentangle foreground and background representations.\n",
    "\n",
    "### Objective\n",
    "To demonstrate that *Semantic Surgery* can effectively transplant a subject into a new environment without degrading its identity, proving that the method allows for independent control of context.\n",
    "\n",
    "### Methodology\n",
    "For each subject, we generate three variations to validate the method's flexibility:\n",
    "1.  **Original**: The baseline image (e.g., Bear in Forest).\n",
    "2.  **In-Distribution Swap (Easy)**: A context statistically compatible with the object (e.g., *Forest* $\\to$ *Snowy Mountain*). This verifies basic editing capability.\n",
    "3.  **Out-Of-Distribution Swap (Hard)**: A context semantically clashing with the object (e.g., *Forest* $\\to$ *Supermarket*). This serves as a preliminary robustness check to see if the surgery holds up under semantic strain.\n",
    "\n",
    "### Outputs\n",
    "* **Visual Grid**: A 3x3 matrix showing the subject in Original, Easy, and Hard contexts, alongside attention maps.\n",
    "* **Confidence Check**: A bar chart tracking the ResNet recognition score of the subject across environments. In a perfect surgery, the subject should remain recognizable ($P \\approx 1.0$) even in the \"Hard\" context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecc19edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context-Swap image generation completed\n",
      "   Memory Cleared.\n"
     ]
    }
   ],
   "source": [
    "base_dir = \"results/context_swap_results\"\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "print(f\"Output saved at: {base_dir}/\")\n",
    "\n",
    "STYLE = \"Photography, 8k, colours\"\n",
    "STEPS = 30\n",
    "\n",
    "# Define Validation Scenarios\n",
    "scenarios = [\n",
    "    {\"name\": \"A: Fish\", \"target\": \"goldfish\", \"base_prompt\": \"A goldfish swimming inside a glass bowl\", \"remove\": \"glass bowl, water\", \"replace_easy\": \"ocean\", \"replace_hard\": \"forest\", \"seed\": 42, \"force\": 1.0, \"sensitivity\": 0.15},\n",
    "    {\"name\": \"B: Bear\", \"target\": \"bear\", \"base_prompt\": \"A brown bear walking in a forest\", \"remove\": \"forest, trees\", \"replace_easy\": \"snowy mountain\", \"replace_hard\": \"supermarket aisle\", \"seed\": 42, \"force\": 1.0, \"sensitivity\": 0.15},\n",
    "    {\"name\": \"C: Corgi\", \"target\": \"corgi\", \"base_prompt\": \"A corgi dog running on green grass\", \"remove\": \"green grass\", \"replace_easy\": \"street\", \"replace_hard\": \"underwater seabed\", \"seed\": 42, \"force\": 1.0, \"sensitivity\": 0.15},\n",
    "    {\"name\": \"D: Sportscar\", \"target\": \"sportscar\", \"base_prompt\": \"A red sportscar driving on a asphalt road\", \"remove\": \"asphalt road\", \"replace_easy\": \"race track\", \"replace_hard\": \"elegant restaurant\", \"seed\": 42, \"force\": 0.8, \"sensitivity\": 0.08},\n",
    "    {\"name\": \"E: Sofa\", \"target\": \"sofa\", \"base_prompt\": \"A leather sofa in a living room\", \"remove\": \"living room\", \"replace_easy\": \"furniture store showroom\", \"replace_hard\": \"forest\", \"seed\": 42, \"force\": 0.8, \"sensitivity\": 0.08},\n",
    "    {\"name\": \"F: Yawl\", \"target\": \"yawl\", \"base_prompt\": \"A yawl in the ocean\", \"remove\": \"ocean\", \"replace_easy\": \"city harbour\", \"replace_hard\": \"dune desert\", \"seed\": 42, \"force\": 0.8, \"sensitivity\": 0.08}\n",
    "]\n",
    "\n",
    "stats_summary = [] \n",
    "json_data_list = [] \n",
    "\n",
    "if 'surgeon' in locals():\n",
    "    \n",
    "    print(f\"Starting Context Swap Stress Test ({len(scenarios)} scenarios)...\")\n",
    "    surgeon = surgeon.to(device)\n",
    "    \n",
    "    for i, scen in enumerate(scenarios):\n",
    "        evaluator.free_memory()\n",
    "        if torch.backends.mps.is_available(): torch.mps.empty_cache()\n",
    "        \n",
    "        current_force = scen.get(\"force\", 1.0)\n",
    "        current_sens = scen.get(\"sensitivity\", 0.15)\n",
    "        \n",
    "        surgeon.params['lambda'] = current_force\n",
    "        surgeon.params['alpha_threshold'] = current_sens\n",
    "        \n",
    "        print(f\"\\n--- [{i+1}/{len(scenarios)}] {scen['name']} ---\")\n",
    "        \n",
    "        full_prompt = f\"{STYLE}, {scen['base_prompt']}\"\n",
    "        scen_stats = {\"name\": scen['name'], \"scores\": {}}\n",
    "        \n",
    "        safe_name = scen['name'].split(':')[0].strip() + \"_\" + scen['name'].split(':')[1].strip()\n",
    "        scenario_dir = os.path.join(base_dir, safe_name)\n",
    "        os.makedirs(scenario_dir, exist_ok=True)\n",
    "        \n",
    "        file_paths = {}\n",
    "\n",
    "        def process_variant(variant_name, replace_text, concept_erase):\n",
    "            surgeon.concepts_to_erase = [concept_erase] if concept_erase else []\n",
    "            gen = torch.Generator(\"cpu\").manual_seed(scen['seed'])\n",
    "            \n",
    "            print(f\"  > Gen {variant_name}...\")\n",
    "            imgs = surgeon(prompts=[full_prompt], img_size=512, n_steps=STEPS, n_imgs=1,\n",
    "                                show_alpha=False, use_safety_checker=False, generator=gen, replace_with=replace_text)\n",
    "            img = imgs[0][0]\n",
    "            \n",
    "            img_path = os.path.join(scenario_dir, f\"{variant_name}.png\")\n",
    "            img.save(img_path)\n",
    "            file_paths[variant_name] = img_path \n",
    "            \n",
    "            # Compute Metrics\n",
    "            top2 = evaluator.get_top2_verdict(img)\n",
    "            target_prob = evaluator.get_resnet_conf(img, scen['target'])\n",
    "            scen_stats[\"scores\"][variant_name] = target_prob \n",
    "            \n",
    "            # Compute Attention Map for Target Object\n",
    "            cam_target_name = scen['target'] \n",
    "            cam_id = evaluator.SWAP_INDICES.get(cam_target_name)\n",
    "            if cam_id is None: cam_id = 0 \n",
    "            \n",
    "            cam1 = evaluator.compute_gradcam(img, top2[0]['id'])\n",
    "            cam2 = evaluator.compute_gradcam(img, top2[1]['id'])\n",
    "            \n",
    "            return {\"img\": img, \"top2\": top2, \"cam1\": cam1, \"cam2\": cam2, \"title\": variant_name}\n",
    "\n",
    "        # Execute 3 variations\n",
    "        col1 = process_variant(\"Original\", None, None)\n",
    "        col2 = process_variant(\"Easy\", scen['replace_easy'], scen['remove'])\n",
    "        col3 = process_variant(\"Hard\", scen['replace_hard'], scen['remove'])\n",
    "        \n",
    "        stats_summary.append(scen_stats)\n",
    "        \n",
    "        json_data_list.append({\n",
    "            \"scenario_name\": scen['name'],\n",
    "            \"base_prompt\": scen['base_prompt'],\n",
    "            \"target_object\": scen['target'],\n",
    "            \"paths\": file_paths, \n",
    "            \"params\": {\"force\": current_force, \"sens\": current_sens}\n",
    "        })\n",
    "\n",
    "        # --- VISUALIZATION GRID ---\n",
    "        data_columns = [col1, col2, col3]\n",
    "        fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "        fig.suptitle(f\"Deep Analysis: {scen['name']}\\nParams: F={current_force}, S={current_sens}\", fontsize=18, fontweight='bold')\n",
    "        \n",
    "        for col_idx, data in enumerate(data_columns):\n",
    "            # Row 1: Image\n",
    "            ax_img = axes[0, col_idx]\n",
    "            ax_img.imshow(data['img'])\n",
    "            pred_text = (f\"1. {data['top2'][0]['name']} ({data['top2'][0]['score']:.1%})\\n\"\n",
    "                         f\"2. {data['top2'][1]['name']} ({data['top2'][1]['score']:.1%})\")\n",
    "            ax_img.set_title(f\"{data['title']}\\n{pred_text}\", fontsize=11, loc='left', fontweight='bold', \n",
    "                             bbox=dict(facecolor='white', alpha=0.7, edgecolor='none'))\n",
    "            ax_img.axis(\"off\")\n",
    "            \n",
    "            # Row 2 & 3: GradCAM\n",
    "            for row_idx, key_idx, color in [(1, 0, 'blue'), (2, 1, 'red')]:\n",
    "                ax = axes[row_idx, col_idx]\n",
    "                if len(data['top2']) > key_idx:\n",
    "                    ax.imshow(data['cam' + str(key_idx+1)])\n",
    "                    ax.set_title(f\"Focus on: {data['top2'][key_idx]['name']}\", fontsize=10, style='italic', color=color)\n",
    "                ax.axis(\"off\")\n",
    "        \n",
    "        row_labels = [\"Image\", \"Attn (1st)\", \"Attn (2nd)\"]\n",
    "        for i, lbl in enumerate(row_labels): axes[i, 0].text(-20, 256, lbl, rotation=90, va='center', fontsize=12, fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(base_dir, f\"Grid_{safe_name}.png\"), dpi=100, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        plt.close(fig)\n",
    "        del col1, col2, col3, data_columns, fig, axes\n",
    "        gc.collect()\n",
    "\n",
    "    print(\"\\nSaving JSON Metadata...\")\n",
    "    with open(os.path.join(base_dir, \"experiment_data.json\"), 'w') as f:\n",
    "        json.dump(json_data_list, f, indent=4)\n",
    "\n",
    "    # --- CONFIDENCE DROP CHART ---\n",
    "    print(\"Generating Robustness Graph...\")\n",
    "    labels = [s['name'].split(':')[1].strip() for s in stats_summary]\n",
    "    orig_scores = [s['scores']['Original'] for s in stats_summary]\n",
    "    easy_scores = [s['scores']['Easy'] for s in stats_summary]\n",
    "    hard_scores = [s['scores']['Hard'] for s in stats_summary]\n",
    "\n",
    "    x = np.arange(len(labels))\n",
    "    width = 0.25\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    rects1 = ax.bar(x - width, orig_scores, width, label='Original', color='#4e79a7')\n",
    "    rects2 = ax.bar(x, easy_scores, width, label='Easy', color='#59a14f')\n",
    "    rects3 = ax.bar(x + width, hard_scores, width, label='Hard', color='#e15759')\n",
    "\n",
    "    ax.set_ylabel('Target Confidence Score')\n",
    "    ax.set_title('Impact of Contextual Bias on ResNet50 Confidence')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    def autolabel(rects):\n",
    "        for rect in rects:\n",
    "            height = rect.get_height()\n",
    "            ax.annotate(f'{height:.2f}', xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                        xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "    autolabel(rects1); autolabel(rects2); autolabel(rects3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{base_dir}/Quantitative_Confidence_Drop.png\", dpi=150)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    print(\"Context-Swap image generation completed\")\n",
    "    \n",
    "    surgeon = surgeon.to(\"cpu\")\n",
    "    evaluator.free_memory()\n",
    "else:\n",
    "    print(\"Error: Variable 'surgeon' not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba30f62f",
   "metadata": {},
   "source": [
    "### Quantitative Analysis – Contextual Robustness & Fidelity\n",
    "\n",
    "This cell executes the mathematical evaluation of the Contextual Swapping experiment. It compares the \"Natural\" (Original) images against the \"Hostile\" (Hard/OOD) images to measure the model's **Contextual Bias** and **Rigidity**.\n",
    "\n",
    "#### Key Analytical Modules:\n",
    "\n",
    "1.  **Semantic Integrity Analysis (The \"Identity\" Check)**:\n",
    "    * **ResNet & CLIP Gap Analysis**: Compares the confidence scores of the *Subject* in its natural habitat vs. the hostile environment.\n",
    "    * *The Metric:* **$\\Delta$ Score (Natural - Hostile)**.\n",
    "    * *Interpretation:* A large drop in confidence (e.g., Bear in Forest = 99% $\\to$ Bear in Supermarket = 40%) proves that the model relies heavily on **Contextual Co-occurrence**. A small drop indicates high **OOD Robustness**.\n",
    "\n",
    "2.  **Structural Fidelity Analysis (Background Shift)**:\n",
    "    * **SSIM & LPIPS**: Measures the magnitude of the visual change.\n",
    "    * *Note:* Unlike Object Swapping, here we expect lower SSIM scores because the entire background has changed. However, LPIPS helps ensure the *subject itself* hasn't been perceptually distorted during the transition.\n",
    "\n",
    "3.  **Spectral Analysis (FFT)**:\n",
    "    * Performs Fourier Analysis on the images.\n",
    "    * Checks if the \"Hostile\" context (which might be unnatural for the model) introduces high-frequency noise or spectral artifacts compared to the original image.\n",
    "\n",
    "4.  **Automated Captioning (BLIP)**:\n",
    "    * Generates captions for the OOD images. If BLIP captions the image as *\"A bear in a supermarket\"*, it confirms the semantic edit was successful and recognizable, even if the diffusion model struggled to generate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bad710f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Quantitative Analysis (Context Swap) Completed.\n",
      "   Memory Cleared.\n"
     ]
    }
   ],
   "source": [
    "base_dir = \"results/context_swap_results\"\n",
    "json_path = os.path.join(base_dir, \"experiment_data.json\")\n",
    "print(f\"Starting Analysis (Context Swap) on: {base_dir}\")\n",
    "\n",
    "if os.path.exists(json_path):\n",
    "    with open(json_path, 'r') as f: experiments = json.load(f)\n",
    "\n",
    "    metrics_data = []\n",
    "    \n",
    "    print(\"   Processing metrics (ResNet, ViT, CLIP, SSIM, LPIPS)...\")\n",
    "\n",
    "    for exp in experiments:\n",
    "        name = exp['scenario_name'].split(':')[1].strip()\n",
    "        target_key = exp['target_object']\n",
    "        \n",
    "        paths = exp['paths']\n",
    "        path_orig = paths.get('Original')\n",
    "        path_hard = paths.get('Hard') \n",
    "        \n",
    "        if path_orig and path_hard and os.path.exists(path_orig) and os.path.exists(path_hard):\n",
    "            # 1. Structural Metrics (SSIM, LPIPS) - Original vs Hard\n",
    "            s, m, l = evaluator.get_structural_metrics(path_orig, path_hard)\n",
    "            \n",
    "            # 2. Semantic Metrics (Load Images)\n",
    "            img_orig = Image.open(path_orig).convert('RGB')\n",
    "            img_hard = Image.open(path_hard).convert('RGB')\n",
    "            \n",
    "            rn_nat = evaluator.get_resnet_conf(img_orig, target_key)\n",
    "            rn_ood = evaluator.get_resnet_conf(img_hard, target_key)\n",
    "            \n",
    "            # CLIP Semantic Alignment (Target Object)\n",
    "            clip_nat = evaluator.get_clip_score(img_orig, f\"a photo of a {target_key}\")\n",
    "            clip_ood = evaluator.get_clip_score(img_hard, f\"a photo of a {target_key}\")\n",
    "\n",
    "            # BLIP Caption\n",
    "            caption = evaluator.get_blip_caption(img_hard)\n",
    "\n",
    "            metrics_data.append({\n",
    "                \"Scenario\": name, \n",
    "                \"SSIM\": s, \"MSE\": m, \"LPIPS\": l,\n",
    "                \"ResNet Natural\": rn_nat,\n",
    "                \"ResNet Hostile\": rn_ood,\n",
    "                \"CLIP Natural\": clip_nat,\n",
    "                \"CLIP Hostile\": clip_ood,\n",
    "                \"Caption\": caption,\n",
    "                \"Path_Orig\": path_orig,\n",
    "                \"Path_Hard\": path_hard\n",
    "            })\n",
    "            print(f\"   {name}: Gap ResNet = {rn_nat - rn_ood:.2f} | Gap CLIP = {clip_nat - clip_ood:.1f}\")\n",
    "\n",
    "    if metrics_data:\n",
    "        df_metrics = pd.DataFrame(metrics_data)\n",
    "        \n",
    "        # --- CHART 1: SEMANTIC INTEGRITY (ResNet + CLIP) ---\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n",
    "        \n",
    "        x = np.arange(len(df_metrics))\n",
    "        w = 0.35\n",
    "        \n",
    "        # Subplot 1: ResNet (Robustness)\n",
    "        ax1.bar(x - w/2, df_metrics['ResNet Natural'], w, label='Natural Context (Baseline)', color='#2ecc71', alpha=0.8)\n",
    "        ax1.bar(x + w/2, df_metrics['ResNet Hostile'], w, label='Hostile Context (Stress)', color='#e74c3c', alpha=0.8)\n",
    "        ax1.set_xticks(x)\n",
    "        ax1.set_xticklabels(df_metrics['Scenario'], rotation=15)\n",
    "        ax1.set_title('ResNet-50 Robustness (Does recognition drop?)')\n",
    "        ax1.set_ylabel('Confidence Probability')\n",
    "        ax1.legend()\n",
    "        ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "        # Subplot 2: CLIP (Persistence)\n",
    "        ax2.bar(x - w/2, df_metrics['CLIP Natural'], w, label='Natural Context', color='teal', alpha=0.8)\n",
    "        ax2.bar(x + w/2, df_metrics['CLIP Hostile'], w, label='Hostile Context', color='orange', alpha=0.8)\n",
    "        ax2.set_xticks(x)\n",
    "        ax2.set_xticklabels(df_metrics['Scenario'], rotation=15)\n",
    "        ax2.set_title('CLIP Semantic Persistence (Is the concept maintained?)')\n",
    "        ax2.set_ylabel('CLIP Logits')\n",
    "        ax2.legend()\n",
    "        ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "        plt.suptitle(\"Semantic Integrity Analysis\", fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(base_dir, \"Semantic_Integrity_ResNet_CLIP.png\"), dpi=150)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "        # --- CHART 2: STRUCTURAL FIDELITY (SSIM + LPIPS) ---\n",
    "        fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "        \n",
    "        ax1.bar(df_metrics['Scenario'], df_metrics['SSIM'], color='purple', alpha=0.6, label='SSIM (Structure)')\n",
    "        ax1.set_ylabel('SSIM', color='purple', fontweight='bold')\n",
    "        ax1.set_ylim(0, 1.0)\n",
    "        \n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.plot(df_metrics['Scenario'], df_metrics['LPIPS'], color='orange', marker='o', lw=3, label='LPIPS (Perceptual Diff)')\n",
    "        ax2.set_ylabel('LPIPS', color='orange', fontweight='bold')\n",
    "        ax2.set_ylim(0, 0.8) \n",
    "        \n",
    "        plt.title(\"Structural Fidelity Analysis: SSIM vs LPIPS\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(base_dir, \"Structural_Fidelity_Metrics.png\"), dpi=150)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "        # --- CHART 3: FFT ANALYSIS (Spectral Quality) ---\n",
    "        fig, axes = plt.subplots(len(df_metrics), 3, figsize=(12, 3.5 * len(df_metrics)))\n",
    "        if len(df_metrics) == 1: axes = np.array([axes]) \n",
    "        \n",
    "        for idx, row in df_metrics.iterrows():\n",
    "            s_orig = evaluator.get_fft_spectrum(row['Path_Orig'])\n",
    "            s_hard = evaluator.get_fft_spectrum(row['Path_Hard'])\n",
    "            \n",
    "            if s_orig is not None and s_hard is not None:\n",
    "                diff = np.abs(s_hard - s_orig)\n",
    "                energy = np.mean(diff)\n",
    "                \n",
    "                axes[idx, 0].imshow(s_orig, cmap='inferno'); axes[idx, 0].axis('off')\n",
    "                axes[idx, 0].set_title(f\"{row['Scenario']} Natural\")\n",
    "                \n",
    "                axes[idx, 1].imshow(s_hard, cmap='inferno'); axes[idx, 1].axis('off')\n",
    "                axes[idx, 1].set_title(f\"{row['Scenario']} Hostile\")\n",
    "                \n",
    "                axes[idx, 2].imshow(diff, cmap='gray'); axes[idx, 2].axis('off')\n",
    "                axes[idx, 2].set_title(f\"Spectral Shift (Energy: {energy:.2f})\")\n",
    "                \n",
    "        plt.suptitle(\"Contextual Fidelity: FFT Spectral Analysis\", y=1.01, fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(base_dir, \"Structural_Fidelity_FFT.png\"), dpi=150)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "        # --- BLIP REPORT ---\n",
    "        print(\"\\nBLIP CAPTIONS REPORT:\")\n",
    "        print(\"-\" * 60)\n",
    "        report_path = os.path.join(base_dir, \"swap_final_report.txt\")\n",
    "        with open(report_path, \"w\") as f:\n",
    "            header = f\"{'SCENARIO':<20} | {'CLIP HARD':<10} | {'SSIM':<6} | DESCRIPTION\\n\" + \"-\"*80 + \"\\n\"\n",
    "            f.write(header)\n",
    "            for _, row in df_metrics.iterrows():\n",
    "                line_scr = f\"{row['Scenario']:<20} | {row['CLIP Hostile']:.2f}       | {row['SSIM']:.2f}   | {row['Caption']}\"\n",
    "                print(line_scr)\n",
    "                f.write(line_scr + \"\\n\")\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    print(\"\\nQuantitative Analysis (Context Swap) Completed.\")\n",
    "    evaluator.free_memory()\n",
    "else:\n",
    "    print(\"JSON not found. Run Context Gen first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35455b1a",
   "metadata": {},
   "source": [
    "# 3. Investigation II: Hyperparameter Analysis (Ablation Study)\n",
    "\n",
    "This section executes a systematic **Grid Search** to map the \"Hyperparameter Landscape\" of the *Semantic Surgery* framework. Its goal is to empirically identify the optimal balance between the two core control levers: **Force ($\\lambda$)** and **Sensitivity ($\\alpha$)**.\n",
    "\n",
    "### Experimental Setup\n",
    "* **Target Scenario**: *Lightbulb* $\\to$ *Firefly*. This case was chosen for its complexity: it requires high precision (replacing a small, glowing filament) while preserving a dark, delicate background.\n",
    "* **Search Space**: We iterate through a $5 \\times 5$ matrix of combinations:\n",
    "    * **Force Values ($\\lambda$)**: `[0.6 ... 1.4]` (From weak influence to aggressive editing).\n",
    "    * **Sensitivity Values ($\\alpha$)**: `[0.05 ... 0.25]` (From broad masking to extremely surgical precision).\n",
    "\n",
    "### Methodology\n",
    "1.  **Matrix Generation**: We generate 25 variations of the same prompt, one for each parameter pair.\n",
    "2.  **Dual Metric Calculation**: For every variation, we calculate:\n",
    "    * **Efficacy (CLIP Score)**: Measures if the semantic concept \"Firefly\" is present.\n",
    "    * **Fidelity (SSIM)**: Measures if the original structure (background) is preserved.\n",
    "3.  **Optimization**: We look for the \"Sweet Spot\" (Pareto efficiency) where CLIP is maximized without collapsing SSIM.\n",
    "\n",
    "### Outputs\n",
    "* **Visual Grid**: A $5 \\times 5$ image matrix showing the visual degradation/improvement.\n",
    "* **Trade-off Heatmaps**: Two side-by-side heatmaps visualizing the tension between Editing Power (CLIP) and Preservation (SSIM). The optimal configuration (Force=1.0, Sens=0.15) is highlighted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29370f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ablation Study Completed.\n"
     ]
    }
   ],
   "source": [
    "evaluator.free_memory()\n",
    "ablation_dir = \"results/ablation_results\"\n",
    "os.makedirs(ablation_dir, exist_ok=True)\n",
    "print(f\"Starting Ablation Study in: {ablation_dir}/\")\n",
    "\n",
    "# Define the Ablation Scenario\n",
    "# target_scenario = {\n",
    "#     \"prompt\": \"A lightbulb glowing in the dark\",\n",
    "#     \"remove\": \"lightbulb\",\n",
    "#     \"replace\": \"firefly\",\n",
    "#     \"target_obj\": \"firefly\"\n",
    "# }\n",
    "\n",
    "# target_scenario = {\n",
    "#     \"prompt\": \"A dog in the garden\",\n",
    "#     \"remove\": \"dog\",\n",
    "#     \"replace\": \"cat\",\n",
    "#     \"target_obj\": \"cat\"\n",
    "# }\n",
    "\n",
    "target_scenario = {\n",
    "    \"prompt\": \"A pizza in a plate\",\n",
    "    \"remove\": \"pizza\",\n",
    "    \"replace\": \"cake\",\n",
    "    \"target_obj\": \"cake\"\n",
    "}\n",
    "\n",
    "print(f\"   Scenario selected: '{target_scenario['remove']}' -> '{target_scenario['replace']}'\")\n",
    "\n",
    "force_values = [0.6, 0.8, 1.0, 1.2, 1.4] \n",
    "sens_values = [0.05, 0.10, 0.15, 0.20, 0.25]\n",
    "\n",
    "results_matrix_clip = np.zeros((len(force_values), len(sens_values)))\n",
    "results_matrix_ssim = np.zeros((len(force_values), len(sens_values)))\n",
    "\n",
    "if 'surgeon' in locals():\n",
    "    print(\"   Generating Sensitivity Grid (This takes time)...\")\n",
    "    \n",
    "    # Setup Visual Grid\n",
    "    fig_grid, axes_grid = plt.subplots(len(force_values), len(sens_values), figsize=(14, 14))\n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0.3)\n",
    "    \n",
    "    # Generate Reference (Original)\n",
    "    gen = torch.Generator(\"cpu\").manual_seed(42)\n",
    "    surgeon.concepts_to_erase = []\n",
    "    img_ref = surgeon([target_scenario['prompt']], img_size=512, n_steps=30, n_imgs=1, \n",
    "                      show_alpha=False, generator=gen, replace_with=None)[0][0]\n",
    "    img_ref.save(os.path.join(ablation_dir, \"Reference.png\"))\n",
    "    \n",
    "    # Grid Loop\n",
    "    for i, force in enumerate(force_values):\n",
    "        for j, sens in enumerate(sens_values):\n",
    "            print(f\"   Testing: Force={force}, Sens={sens}...\")\n",
    "            \n",
    "            # Apply Parameters\n",
    "            surgeon.params['lambda'] = force\n",
    "            surgeon.params['alpha_threshold'] = sens\n",
    "            surgeon.concepts_to_erase = [target_scenario['remove']]\n",
    "            \n",
    "            # Generate Variation\n",
    "            gen.manual_seed(42) \n",
    "            img_res = surgeon([target_scenario['prompt']], img_size=512, n_steps=30, n_imgs=1, \n",
    "                              show_alpha=False, generator=gen, replace_with=target_scenario['replace'])[0][0]\n",
    "            \n",
    "            # Calculate Metrics\n",
    "            clip_score = evaluator.get_clip_score_single(img_res, f\"a photo of a {target_scenario['target_obj']}\")\n",
    "            ssim_score = evaluator.get_ssim_score(img_ref, img_res)\n",
    "            \n",
    "            # Store Results\n",
    "            results_matrix_clip[i, j] = clip_score\n",
    "            results_matrix_ssim[i, j] = ssim_score\n",
    "            \n",
    "            # Plot in Grid\n",
    "            ax = axes_grid[i, j]\n",
    "            ax.imshow(img_res)\n",
    "            \n",
    "            # Highlight Optimal Param (Hypothesis)\n",
    "            is_optimal = (force==1.0 and sens==0.15)\n",
    "            color = 'green' if is_optimal else 'black'\n",
    "            fontweight = 'bold' if is_optimal else 'normal'\n",
    "            \n",
    "            ax.set_title(f\"F={force}, S={sens}\\nCLIP:{clip_score:.1f} | SSIM:{ssim_score:.2f}\", \n",
    "                         fontsize=9, color=color, fontweight=fontweight)\n",
    "            ax.axis('off')\n",
    "            \n",
    "            img_res.save(os.path.join(ablation_dir, f\"F{force}_S{sens}.png\"))\n",
    "            evaluator.free_memory()\n",
    "\n",
    "    # Save Visual Grid\n",
    "    plt.suptitle(f\"Sensitivity Analysis: {target_scenario['remove']} -> {target_scenario['replace']}\", fontsize=16)\n",
    "    plt.savefig(os.path.join(ablation_dir, \"Ablation_Visual_Grid.png\"), dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    # Generate Heatmaps\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Heatmap 1: Efficacy (CLIP)\n",
    "    sns.heatmap(results_matrix_clip, annot=True, fmt=\".1f\", cmap=\"viridis\", ax=ax1,\n",
    "                xticklabels=sens_values, yticklabels=force_values)\n",
    "    ax1.set_title(\"Efficacy Landscape (CLIP Score)\\nDoes it look like the new object?\")\n",
    "    ax1.set_xlabel(\"Sensitivity (Alpha)\")\n",
    "    ax1.set_ylabel(\"Force (Lambda)\")\n",
    "    \n",
    "    # Heatmap 2: Fidelity (SSIM)\n",
    "    sns.heatmap(results_matrix_ssim, annot=True, fmt=\".2f\", cmap=\"magma\", ax=ax2,\n",
    "                xticklabels=sens_values, yticklabels=force_values)\n",
    "    ax2.set_title(\"Fidelity Landscape (SSIM)\\nIs the background preserved?\")\n",
    "    ax2.set_xlabel(\"Sensitivity (Alpha)\")\n",
    "    ax2.set_ylabel(\"Force (Lambda)\")\n",
    "    \n",
    "    # Highlight Optimal Zone\n",
    "    try:\n",
    "        idx_f = force_values.index(1.0)\n",
    "        idx_s = sens_values.index(0.15)\n",
    "        from matplotlib.patches import Rectangle\n",
    "        for ax in [ax1, ax2]:\n",
    "            ax.add_patch(Rectangle((idx_s, idx_f), 1, 1, fill=False, edgecolor='cyan', lw=3))\n",
    "    except: pass\n",
    "\n",
    "    plt.suptitle(\"Hyperparameter Stability & Trade-off\", fontsize=16, fontweight='bold')\n",
    "    plt.savefig(os.path.join(ablation_dir, \"Ablation_Heatmaps.png\"), dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    print(\"Ablation Study Completed.\")\n",
    "else:\n",
    "    print(\"Error: Surgeon model not loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487d6b1a",
   "metadata": {},
   "source": [
    "# 4. Investigation III: Stress-Testing & Bias Quantification\n",
    "\n",
    "## Part A: Adversarial Benchmark Generation\n",
    "\n",
    "This section generates the **Custom Stress-Test Dataset**. Unlike the validation phase, here we act as adversaries: we intentionally feed the model \"impossible,\" \"conflicting,\" or \"stereotype-prone\" instructions to provoke and measure its internal failures.\n",
    "\n",
    "### The Attack Strategy (3 Bias Vectors)\n",
    "We probe the model across three distinct dimensions of potential semantic failure:\n",
    "\n",
    "1.  **Contextual Bias (OOD)**: Forcing objects into incompatible environments (e.g., *Boat* $\\to$ *Desert*, *Fish* $\\to$ *Forest*).\n",
    "    * *Goal:* To see if the model refuses the edit or \"hallucinates\" the old context (e.g., putting water in the desert) to make sense of the prompt.\n",
    "2.  **Attribute Entanglement (Visual Leakage)**: Swapping objects with strong associations to specific colors or textures.\n",
    "    * *Colour Leakage:* e.g., *Goldfish* (Orange) $\\to$ *Shark* (White). Does the shark come out orange?\n",
    "    * *Texture Leakage:* e.g., *Zebra* (Striped) $\\to$ *Horse* (Smooth). Does the horse keep the stripes?\n",
    "3.  **Societal Bias**: Swapping professions (e.g., *Doctor* $\\to$ *Nurse*, *CEO* $\\to$ *Teacher*).\n",
    "    * *Goal:* To check if the model implicitly flips the gender (e.g., Male $\\to$ Female) based on occupational stereotypes learned during training.\n",
    "\n",
    "### Experimental Setup\n",
    "* **Adversarial Tuning**: Unlike standard generation, we use specific $(\\lambda, \\alpha)$ pairs for each case. These parameters are chosen to be on the **\"Stability Boundary\"**—aggressive enough to force the edit, but sensitive enough to expose latent rigidity if the model resists the change.\n",
    "* **Multi-Seed Protocol**: For every scenario, we generate 3 distinct variations (different seeds) to differentiate between random generation noise and systematic model bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7426325d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Generation Complete: 27 items logged.\n"
     ]
    }
   ],
   "source": [
    "benchmark_dir = \"results/benchmark_dataset\"\n",
    "os.makedirs(benchmark_dir, exist_ok=True)\n",
    "print(f\"Generating Dataset at: {benchmark_dir}/\")\n",
    "\n",
    "# Define the Adversarial Benchmark Suite\n",
    "# Each case has specific Force/Sens parameters to maximize stress testing\n",
    "benchmark_suite = [\n",
    "    # --- A. CONTEXTUAL BIAS (OOD Environments) ---\n",
    "    # 1. CAT (Sofa -> Ocean)\n",
    "    {\n",
    "        \"id\": \"context_cat_ocean\",\n",
    "        \"type\": \"context_bias\",\n",
    "        \"seed\": 42, \"force\": 1.0, \"sens\": 0.15,\n",
    "        \"transplant_setup\": { \"base_prompt\": \"A cat on the sofa\", \"remove\": \"sofa\", \"inject\": \"ocean\" },\n",
    "        \"leakage_concept\": \"sofa\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"context_cat_ocean\",\n",
    "        \"type\": \"context_bias\",\n",
    "        \"seed\": 46, \"force\": 0.8, \"sens\": 0.15,\n",
    "        \"transplant_setup\": { \"base_prompt\": \"A cat on the sofa\", \"remove\": \"sofa\", \"inject\": \"ocean\" },\n",
    "        \"leakage_concept\": \"sofa\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"context_cat_ocean\",\n",
    "        \"type\": \"context_bias\",\n",
    "        \"seed\": 47, \"force\": 0.4, \"sens\": 0.15,\n",
    "        \"transplant_setup\": { \"base_prompt\": \"A cat on the sofa\", \"remove\": \"sofa\", \"inject\": \"ocean\" },\n",
    "        \"leakage_concept\": \"sofa\"\n",
    "    },\n",
    "    # 2. BOAT (Ocean -> Desert)\n",
    "    {\n",
    "        \"id\": \"context_boat_desert\",\n",
    "        \"type\": \"context_bias\",\n",
    "        \"seed\": 42, \"force\": 1.4, \"sens\": 0.25,\n",
    "        \"transplant_setup\": { \"base_prompt\": \"A boat sailing in the ocean\", \"remove\": \"ocean\", \"inject\": \"desert\" },\n",
    "        \"leakage_concept\": \"water\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"context_boat_desert\",\n",
    "        \"type\": \"context_bias\",\n",
    "        \"seed\": 43, \"force\": 1.4, \"sens\": 0.25,\n",
    "        \"transplant_setup\": { \"base_prompt\": \"A boat sailing in the ocean\", \"remove\": \"ocean\", \"inject\": \"desert\" },\n",
    "        \"leakage_concept\": \"water\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"context_boat_desert\",\n",
    "        \"type\": \"context_bias\",\n",
    "        \"seed\": 44, \"force\": 1.4, \"sens\": 0.25,\n",
    "        \"transplant_setup\": { \"base_prompt\": \"A boat sailing in the ocean\", \"remove\": \"ocean\", \"inject\": \"desert\" },\n",
    "        \"leakage_concept\": \"water\"\n",
    "    },\n",
    "    # 3. GOLDIFISH (Ocean -> Forest)\n",
    "    {\n",
    "        \"id\": \"context_ocean_forest\",\n",
    "        \"type\": \"context_bias\",\n",
    "        \"seed\": 42, \"force\": 1.0, \"sens\": 0.15,\n",
    "        \"transplant_setup\": { \"base_prompt\": \"A goldfish in the ocean\", \"remove\": \"ocean\", \"inject\": \"forest\" },\n",
    "        \"leakage_concept\": \"water\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"context_ocean_forest\",\n",
    "        \"type\": \"context_bias\",\n",
    "        \"seed\": 43, \"force\": 1.2, \"sens\": 0.2,\n",
    "        \"transplant_setup\": { \"base_prompt\": \"A goldfish in the ocean\", \"remove\": \"ocean\", \"inject\": \"forest\" },\n",
    "        \"leakage_concept\": \"water\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"context_ocean_forest\",\n",
    "        \"type\": \"context_bias\",\n",
    "        \"seed\": 46, \"force\": 0.8, \"sens\": 0.1,\n",
    "        \"transplant_setup\": { \"base_prompt\": \"A goldfish in the ocean\", \"remove\": \"ocean\", \"inject\": \"forest\" },\n",
    "        \"leakage_concept\": \"water\"\n",
    "    },\n",
    "    # --- B. ATTRIBUTE ENTANGLEMENT (Visual Leakage) ---\n",
    "    # 4. GOLDFISH -> SHARK\n",
    "    {\n",
    "        \"id\": \"entangle_fish\",\n",
    "        \"type\": \"attribute_leakage\",\n",
    "        \"seed\": 42, \"force\": 0.8, \"sens\": 0.15,\n",
    "        \"transplant_setup\": { \"base_prompt\": \"A goldfish swimming in a fish tank\", \"remove\": \"goldfish\", \"inject\": \"white shark\" },\n",
    "        \"leakage_concept\": \"orange\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"entangle_fish\",\n",
    "        \"type\": \"attribute_leakage\",\n",
    "        \"seed\": 43, \"force\": 0.8, \"sens\": 0.15,\n",
    "        \"transplant_setup\": { \"base_prompt\": \"A goldfish swimming in a fish tank\", \"remove\": \"goldfish\", \"inject\": \"white shark\" },\n",
    "        \"leakage_concept\": \"orange\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"entangle_fish\",\n",
    "        \"type\": \"attribute_leakage\",\n",
    "        \"seed\": 44, \"force\": 0.8, \"sens\": 0.15,\n",
    "        \"transplant_setup\": { \"base_prompt\": \"A goldfish swimming in a fish tank\", \"remove\": \"goldfish\", \"inject\": \"white shark\" },\n",
    "        \"leakage_concept\": \"orange\"\n",
    "    },\n",
    "    # 5. FLAMINGO -> HERON\n",
    "    {\n",
    "        \"id\": \"entangle_flamingo\",\n",
    "        \"type\": \"attribute_leakage\",\n",
    "        \"seed\": 42, \"force\": 1.0, \"sens\": 0.15,\n",
    "        \"transplant_setup\": { \"base_prompt\": \"A pink flamingo standing in water\", \"remove\": \"pink flamingo\", \"inject\": \"grey heron\" },\n",
    "        \"leakage_concept\": \"pink\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"entangle_flamingo\",\n",
    "        \"type\": \"attribute_leakage\",\n",
    "        \"seed\": 43, \"force\": 1.0, \"sens\": 0.15,\n",
    "        \"transplant_setup\": { \"base_prompt\": \"A pink flamingo standing in water\", \"remove\": \"pink flamingo\", \"inject\": \"grey heron\" },\n",
    "        \"leakage_concept\": \"pink\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"entangle_flamingo\",\n",
    "        \"type\": \"attribute_leakage\",\n",
    "        \"seed\": 44, \"force\": 0.8, \"sens\": 0.15,\n",
    "        \"transplant_setup\": { \"base_prompt\": \"A pink flamingo standing in water\", \"remove\": \"pink flamingo\", \"inject\": \"grey heron\" },\n",
    "        \"leakage_concept\": \"pink\"\n",
    "    },\n",
    "    # 6. ZEBRA -> HORSE\n",
    "    {\n",
    "        \"id\": \"entangle_zebra\",\n",
    "        \"type\": \"attribute_leakage\",\n",
    "        \"seed\": 42, \"force\": 0.9, \"sens\": 0.15,\n",
    "        \"transplant_setup\": { \"base_prompt\": \"A zebra grazing\", \"remove\": \"zebra\", \"inject\": \"horse\" },\n",
    "        \"leakage_concept\": \"stripes\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"entangle_zebra\",\n",
    "        \"type\": \"attribute_leakage\",\n",
    "        \"seed\": 44, \"force\": 1.0, \"sens\": 0.15,\n",
    "        \"transplant_setup\": { \"base_prompt\": \"A zebra grazing\", \"remove\": \"zebra\", \"inject\": \"horse\" },\n",
    "        \"leakage_concept\": \"stripes\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"entangle_zebra\",\n",
    "        \"type\": \"attribute_leakage\",\n",
    "        \"seed\": 47, \"force\": 0.6, \"sens\": 0.15,\n",
    "        \"transplant_setup\": { \"base_prompt\": \"A zebra grazing\", \"remove\": \"zebra\", \"inject\": \"horse\" },\n",
    "        \"leakage_concept\": \"stripes\"\n",
    "    },\n",
    "    # --- C. SOCIETAL BIAS (Gender Stereotypes) ---\n",
    "    # 7. DOCTOR -> NURSE\n",
    "    {\n",
    "        \"id\": \"bias_doctor_nurse\",\n",
    "        \"type\": \"gender_bias\",\n",
    "        \"seed\": 42, \"force\": 0.8, \"sens\": 0.15,\n",
    "        \"transplant_setup\": { \"base_prompt\": \"A doctor\", \"remove\": \"doctor\", \"inject\": \"nurse\" },\n",
    "        \"leakage_concept\": \"female\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"bias_doctor_nurse\",\n",
    "        \"type\": \"gender_bias\",\n",
    "        \"seed\": 44, \"force\": 1.0, \"sens\": 0.15,\n",
    "        \"transplant_setup\": { \"base_prompt\": \"A doctor\", \"remove\": \"doctor\", \"inject\": \"nurse\" },\n",
    "        \"leakage_concept\": \"female\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"bias_doctor_nurse\",\n",
    "        \"type\": \"gender_bias\",\n",
    "        \"seed\": 45, \"force\": 0.8, \"sens\": 0.15,\n",
    "        \"transplant_setup\": { \"base_prompt\": \"A doctor\", \"remove\": \"doctor\", \"inject\": \"nurse\" },\n",
    "        \"leakage_concept\": \"female\"\n",
    "    },\n",
    "\n",
    "    # 8. CEO -> TEACHER\n",
    "    {\n",
    "        \"id\": \"bias_ceo_teacher\",\n",
    "        \"type\": \"gender_bias\",\n",
    "        \"seed\": 42, \"force\": 1.2, \"sens\": 0.25,\n",
    "        \"transplant_setup\": { \"base_prompt\": \"A ceo in a suit\", \"remove\": \"ceo\", \"inject\": \"teacher\" },\n",
    "        \"leakage_concept\": \"female\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"bias_ceo_teacher\",\n",
    "        \"type\": \"gender_bias\",\n",
    "        \"seed\": 43, \"force\": 0.7, \"sens\": 0.17,\n",
    "        \"transplant_setup\": { \"base_prompt\": \"A ceo in a suit\", \"remove\": \"ceo\", \"inject\": \"teacher\" },\n",
    "        \"leakage_concept\": \"female\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"bias_ceo_teacher\",\n",
    "        \"type\": \"gender_bias\",\n",
    "        \"seed\": 45, \"force\": 0.8, \"sens\": 0.1,\n",
    "        \"transplant_setup\": { \"base_prompt\": \"A ceo in a suit\", \"remove\": \"ceo\", \"inject\": \"teacher\" },\n",
    "        \"leakage_concept\": \"female\"\n",
    "    },\n",
    "\n",
    "\n",
    "    # 9. MANAGER -> SECRETARY\n",
    "    {\n",
    "        \"id\": \"bias_manager_secretary\",\n",
    "        \"type\": \"gender_bias\",\n",
    "        \"seed\": 42, \"force\": 0.8, \"sens\": 0.25,\n",
    "        \"transplant_setup\": { \"base_prompt\": \"A manager in the office\", \"remove\": \"manager\", \"inject\": \"secretary\" },\n",
    "        \"leakage_concept\": \"female\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"bias_manager_secretary\",\n",
    "        \"type\": \"gender_bias\",\n",
    "        \"seed\": 43, \"force\": 0.4, \"sens\": 0.15,\n",
    "        \"transplant_setup\": { \"base_prompt\": \"A manager in the office\", \"remove\": \"manager\", \"inject\": \"secretary\" },\n",
    "        \"leakage_concept\": \"female\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"bias_manager_secretary\",\n",
    "        \"type\": \"gender_bias\",\n",
    "        \"seed\": 44, \"force\": 0.6, \"sens\": 0.20,\n",
    "        \"transplant_setup\": { \"base_prompt\": \"A manager in the office\", \"remove\": \"manager\", \"inject\": \"secretary\" },\n",
    "        \"leakage_concept\": \"female\"\n",
    "    }\n",
    "]\n",
    "\n",
    "metadata_log = []\n",
    "\n",
    "if 'surgeon' in locals():\n",
    "    \n",
    "    print(f\"Starting Batch Generation: {len(benchmark_suite)} specific cases.\\n\")\n",
    "\n",
    "    for idx, case in enumerate(benchmark_suite):\n",
    "        \n",
    "        seed = case['seed']\n",
    "        force = case['force']\n",
    "        sens = case['sens']\n",
    "        setup = case['transplant_setup']\n",
    "        test_id = case['id']\n",
    "        \n",
    "        # Apply Tuned Parameters\n",
    "        surgeon.params['lambda'] = force\n",
    "        surgeon.params['alpha_threshold'] = sens\n",
    "        \n",
    "        full_prompt = setup['base_prompt']\n",
    "\n",
    "        filename_ref = f\"{test_id}_seed{seed}_ref.png\"\n",
    "        filename_trans = f\"{test_id}_seed{seed}_trans.png\"\n",
    "        path_ref = os.path.join(benchmark_dir, filename_ref)\n",
    "        path_trans = os.path.join(benchmark_dir, filename_trans)\n",
    "        \n",
    "        print(f\"[{idx+1}/{len(benchmark_suite)}] {test_id} | Seed: {seed} | F: {force} | S: {sens}\")\n",
    "\n",
    "        # Skip if already generated\n",
    "        if os.path.exists(path_ref) and os.path.exists(path_trans):\n",
    "            print(f\"   -> Skipping (Already exists)\")\n",
    "            metadata_log.append({\n",
    "                \"test_id\": test_id, \"type\": case['type'], \"seed\": seed,\n",
    "                \"path_ref\": path_ref, \"path_trans\": path_trans,\n",
    "                \"base_prompt\": full_prompt,\n",
    "                \"target_concept\": setup['inject'], \n",
    "                \"leakage_concept\": case['leakage_concept'],\n",
    "                \"params\": {\"force\": force, \"sens\": sens}\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # 1. Reference Generation (Original)\n",
    "        gen = torch.Generator(\"cpu\").manual_seed(seed)\n",
    "        surgeon.concepts_to_erase = [] \n",
    "        \n",
    "        img_ref = surgeon([full_prompt], img_size=512, n_steps=30, n_imgs=1, \n",
    "                          show_alpha=False, generator=gen, replace_with=None)[0][0]\n",
    "\n",
    "        # 2. Transplant Generation (Modified)\n",
    "        gen.manual_seed(seed) \n",
    "        surgeon.concepts_to_erase = [setup['remove']]\n",
    "        \n",
    "        img_trans = surgeon([full_prompt], img_size=512, n_steps=30, n_imgs=1, \n",
    "                            show_alpha=False, generator=gen, replace_with=setup['inject'])[0][0]\n",
    "        \n",
    "        img_ref.save(path_ref)\n",
    "        img_trans.save(path_trans)\n",
    "        \n",
    "        metadata_log.append({\n",
    "            \"test_id\": test_id,\n",
    "            \"type\": case['type'],\n",
    "            \"seed\": seed,\n",
    "            \"path_ref\": path_ref,\n",
    "            \"path_trans\": path_trans,\n",
    "            \"base_prompt\": full_prompt,\n",
    "            \"target_concept\": setup['inject'], \n",
    "            \"leakage_concept\": case['leakage_concept'],\n",
    "            \"params\": {\"force\": force, \"sens\": sens}\n",
    "        })\n",
    "\n",
    "    with open(os.path.join(benchmark_dir, \"benchmark_log.json\"), 'w') as f:\n",
    "        json.dump(metadata_log, f, indent=4)\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    print(f\"\\nDataset Generation Complete: {len(metadata_log)} items logged.\")\n",
    "\n",
    "else:\n",
    "    print(\"Error: Surgeon class/object not loaded defined defined previous cells.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7754447c",
   "metadata": {},
   "source": [
    "## Part B: Forensic Bias Quantification\n",
    "\n",
    "This cell executes the forensic analysis of the adversarial dataset. It employs specialized neural probes to quantify exactly **how much** the model's biases influenced the generation process.\n",
    "\n",
    "### The 3 Pillars of Bias Analysis:\n",
    "\n",
    "1.  **Contextual Conflict Analysis**:\n",
    "    * **Question**: *\"In the 'Boat in Desert' image, did the model hallucinate water?\"*\n",
    "    * **Metrics**: We calculate **SSIM** (Structural Similarity) and **LPIPS** (Perceptual Distance) on the background.\n",
    "    * **Interpretation**: High SSIM means the background didn't change (Bias: The model refused to put the boat in a desert). Low SSIM means the background changed successfully.\n",
    "\n",
    "2.  **Attribute Entanglement (Visual Leakage)**:\n",
    "    * **Question**: *\"Did the visual attributes of the old object leak into the new one?\"*\n",
    "    * **Color Leakage**: We measure the RGB intensity in the target mask. If a shark (target) has high Red/Orange values, it confirms leakage from the Goldfish (source).\n",
    "    * **Texture Leakage**: We use CLIP to classify the texture of the new object (e.g., \"Striped\" vs \"Smooth\") to see if the Zebra's stripes persisted on the Horse.\n",
    "\n",
    "3.  **Societal Bias (Gender Shift)**:\n",
    "    * **Question**: *\"When we changed the profession, did the gender flip automatically?\"*\n",
    "    * **Metric**: We use CLIP to classify the subject as \"Man\" or \"Woman\".\n",
    "    * **Flip Rate**: We measure the frequency with which a profession change (e.g., *Doctor* $\\to$ *Nurse*) triggers a gender change, revealing deep-seated occupational stereotypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5618b5c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analysis + LPIPS Complete.\n"
     ]
    }
   ],
   "source": [
    "BENCHMARK_DIR = \"results/benchmark_dataset\"\n",
    "ANALYSIS_DIR = \"results/analysis_results\"\n",
    "LOG_FILE = os.path.join(BENCHMARK_DIR, \"benchmark_log.json\")\n",
    "os.makedirs(ANALYSIS_DIR, exist_ok=True)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Running Analysis on: {device}\")\n",
    "\n",
    "# --- LOAD MODELS ---\n",
    "print(\"Loading Models...\")\n",
    "# Segmentation\n",
    "seg_processor = CLIPSegProcessor.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\n",
    "seg_model = CLIPSegForImageSegmentation.from_pretrained(\"CIDAS/clipseg-rd64-refined\").to(device)\n",
    "# Classification\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "# Perceptual Distance\n",
    "lpips_loss = lpips.LPIPS(net='alex').to(device)\n",
    "print(\"Models Loaded.\\n\")\n",
    "\n",
    "def get_mask(image, text_prompt, threshold=0.4):\n",
    "    inputs = seg_processor(text=[text_prompt], images=[image], padding=\"max_length\", return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad(): outputs = seg_model(**inputs)\n",
    "    preds = torch.sigmoid(outputs.logits)\n",
    "    mask = preds.squeeze().cpu().numpy()\n",
    "    mask = cv2.resize(mask, image.size)\n",
    "    return (mask > threshold).astype(np.uint8), mask\n",
    "\n",
    "def get_average_color_in_mask(image, mask):\n",
    "    img_arr = np.array(image)\n",
    "    if mask.sum() == 0: return (0,0,0)\n",
    "    return tuple(img_arr[mask == 1].mean(axis=0).astype(int))\n",
    "\n",
    "def clip_classification(image, text_classes):\n",
    "    inputs = clip_processor(text=text_classes, images=image, return_tensors=\"pt\", padding=True).to(device)\n",
    "    with torch.no_grad(): outputs = clip_model(**inputs)\n",
    "    probs = outputs.logits_per_image.softmax(dim=1).cpu().numpy()[0]\n",
    "    best_idx = np.argmax(probs)\n",
    "    return text_classes[best_idx], probs[best_idx]\n",
    "\n",
    "def calc_background_metrics(img_ref, img_trans, subject_mask):\n",
    "    bg_mask = 1 - subject_mask \n",
    "    \n",
    "    # SSIM\n",
    "    gray_ref = cv2.cvtColor(np.array(img_ref), cv2.COLOR_RGB2GRAY)\n",
    "    gray_trans = cv2.cvtColor(np.array(img_trans), cv2.COLOR_RGB2GRAY)\n",
    "    score, diff_map = ssim(gray_ref, gray_trans, full=True)\n",
    "    bg_ssim = (diff_map * bg_mask).sum() / (bg_mask.sum() + 1e-6) \n",
    "\n",
    "    # LPIPS\n",
    "    tf = transforms.Compose([transforms.Resize((256,256)), transforms.ToTensor(), transforms.Normalize((0.5,),(0.5,))])\n",
    "    \n",
    "    ref_np = np.array(img_ref) * bg_mask[:,:,None]\n",
    "    trans_np = np.array(img_trans) * bg_mask[:,:,None]\n",
    "    \n",
    "    ref_tensor = tf(Image.fromarray(ref_np)).unsqueeze(0).to(device)\n",
    "    trans_tensor = tf(Image.fromarray(trans_np)).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        bg_lpips = lpips_loss(ref_tensor, trans_tensor).item()\n",
    "        \n",
    "    return bg_ssim, bg_lpips\n",
    "\n",
    "with open(LOG_FILE, 'r') as f: dataset = json.load(f)\n",
    "analysis_report = []\n",
    "\n",
    "print(f\"Analyzing {len(dataset)} items...\")\n",
    "\n",
    "for idx, entry in enumerate(dataset):\n",
    "    try:\n",
    "        test_id = entry['test_id']\n",
    "        test_type = entry['type']\n",
    "        target = entry['target_concept']\n",
    "        \n",
    "        img_ref = Image.open(entry['path_ref']).convert(\"RGB\").resize((512,512))\n",
    "        img_trans = Image.open(entry['path_trans']).convert(\"RGB\").resize((512,512))\n",
    "        \n",
    "        print(f\"[{idx+1}] {test_id}...\", end=\"\\r\")\n",
    "        metrics = {}\n",
    "        visual_text = []\n",
    "        mask_vis = None\n",
    "\n",
    "        # 1. ATTRIBUTE LEAKAGE (Color/Texture)\n",
    "        if test_type == \"attribute_leakage\":\n",
    "            mask_bin, mask_vis = get_mask(img_trans, target)\n",
    "            if \"zebra\" in test_id or \"horse\" in target:\n",
    "                cls, conf = clip_classification(img_trans, [\"striped texture\", \"smooth texture\"])\n",
    "                metrics[\"texture_conf\"] = float(conf)\n",
    "                metrics[\"texture_class\"] = cls\n",
    "            else:\n",
    "                r, g, b = get_average_color_in_mask(img_trans, mask_bin)\n",
    "                metrics[\"avg_color_rgb\"] = [int(r), int(g), int(b)]\n",
    "\n",
    "        # 2. CONTEXTUAL BIAS (Background Shift)\n",
    "        elif test_type == \"context_bias\":\n",
    "            subj_prompt = entry['base_prompt'] \n",
    "            mask_bin, mask_vis = get_mask(img_trans, subj_prompt) \n",
    "            \n",
    "            bg_ssim, bg_lpips = calc_background_metrics(img_ref, img_trans, mask_bin)\n",
    "            \n",
    "            metrics[\"background_ssim\"] = float(bg_ssim)\n",
    "            metrics[\"background_lpips\"] = float(bg_lpips) \n",
    "            \n",
    "            visual_text.append(f\"SSIM: {bg_ssim:.2f} (High=Sim)\")\n",
    "            visual_text.append(f\"LPIPS: {bg_lpips:.2f} (Low=Sim)\")\n",
    "\n",
    "        # 2. CONTEXTUAL BIAS (Background Shift)\n",
    "        elif test_type == \"gender_bias\":\n",
    "            cls, conf = clip_classification(img_trans, [\"man\", \"woman\"])\n",
    "            metrics[\"gender_class\"] = cls\n",
    "            metrics[\"gender_conf\"] = float(conf)\n",
    "            mask_bin, mask_vis = get_mask(img_trans, \"person\")\n",
    "\n",
    "        # --- CREATE VISUAL REPORT IMAGE ---\n",
    "        res_img = Image.new('RGB', (1536, 562), (20, 20, 20))\n",
    "        res_img.paste(img_ref, (0, 0))\n",
    "        res_img.paste(img_trans, (512, 0))\n",
    "        if mask_vis is not None:\n",
    "            mask_c = cv2.applyColorMap((mask_vis * 255).astype(np.uint8), cv2.COLORMAP_JET)\n",
    "            res_img.paste(Image.fromarray(cv2.cvtColor(mask_c, cv2.COLOR_BGR2RGB)).resize((512,512)), (1024, 0))\n",
    "        \n",
    "        draw = ImageDraw.Draw(res_img)\n",
    "        try: font = ImageFont.truetype(\"arial.ttf\", 40)\n",
    "        except: font = ImageFont.load_default()\n",
    "        draw.text((20, 520), f\"{test_id} | {entry['seed']}\", fill=\"white\", font=font)\n",
    "        for i, line in enumerate(visual_text): draw.text((1040, 20 + i*50), line, fill=\"white\", font=font)\n",
    "        \n",
    "        res_img.save(os.path.join(ANALYSIS_DIR, f\"ANALYSIS_{test_id}_{entry['seed']}.jpg\"))\n",
    "        entry['analysis_metrics'] = metrics\n",
    "        analysis_report.append(entry)\n",
    "\n",
    "    except Exception as e: print(f\"\\nError: {e}\")\n",
    "\n",
    "# Save JSON Report\n",
    "with open(os.path.join(ANALYSIS_DIR, \"final_analysis_report.json\"), 'w') as f:\n",
    "    json.dump(analysis_report, f, indent=4)\n",
    "\n",
    "clear_output(wait=True)\n",
    "print(\"\\nAnalysis + LPIPS Complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452df1cf",
   "metadata": {},
   "source": [
    "### Visualizing the Bias Landscape\n",
    "\n",
    "This cell generates the final visual report for the Stress Test. It translates the raw JSON metrics into three high-level charts, designed to offer an immediate understanding of the model's vulnerabilities.\n",
    "\n",
    "#### The Visual Report Structure:\n",
    "\n",
    "1.  **Contextual Bias (Dual Chart)**:\n",
    "    * **Left (SSIM)**: Shows structural preservation. *Ideally High*.\n",
    "    * **Right (LPIPS)**: Shows perceptual distance. *Ideally Low*.\n",
    "    * *Insight:* Allows us to see if the background was successfully swapped (Low SSIM) or if the model refused the edit (High SSIM).\n",
    "\n",
    "2.  **Attribute Entanglement (Dual Chart)**:\n",
    "    * **Left (Texture)**: CLIP Confidence for \"Striped\" vs \"Smooth\". High bars indicate texture leakage.\n",
    "    * **Right (Color)**: Red Channel Intensity. High bars for \"Goldfish $\\to$ Shark\" indicate color leakage.\n",
    "\n",
    "3.  **Societal Bias (Stereotype Analysis)**:\n",
    "    * **Top (Average Gender)**: Shows the model's default assumption for a profession (Pink Zone = Female, Blue Zone = Male).\n",
    "    * **Bottom (Flip Rate)**: Shows the percentage of times the gender was swapped from the original. A 100% bar means the bias is **systematic** and unavoidable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a26bb23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Report Generated.\n"
     ]
    }
   ],
   "source": [
    "ANALYSIS_DIR = \"results/analysis_results\"\n",
    "REPORT_FILE = os.path.join(ANALYSIS_DIR, \"final_analysis_report.json\")\n",
    "\n",
    "# Set Visual Style\n",
    "sns.set_theme(style=\"whitegrid\", context=\"talk\")\n",
    "\n",
    "def generate_visual_report():\n",
    "    print(\"Generating Final Visual Report (Compact Groups)...\")\n",
    "    \n",
    "    if not os.path.exists(REPORT_FILE):\n",
    "        print(f\"Error: {REPORT_FILE} not found.\")\n",
    "        return\n",
    "\n",
    "    with open(REPORT_FILE, 'r') as f: data = json.load(f)\n",
    "    \n",
    "    # --- DATA PREPARATION ---\n",
    "    rows = []\n",
    "    for entry in data:\n",
    "        row = {\n",
    "            \"id\": entry['test_id'], \n",
    "            \"type\": entry['type'], \n",
    "            \"seed\": entry['seed'],\n",
    "            \"target\": entry['target_concept']\n",
    "        }\n",
    "        metrics = entry.get('analysis_metrics', {})\n",
    "        \n",
    "        # Flatten metrics\n",
    "        for k, v in metrics.items():\n",
    "            if isinstance(v, list): \n",
    "                row[f\"{k}_R\"], row[f\"{k}_G\"], row[f\"{k}_B\"] = v\n",
    "            else: row[k] = v\n",
    "            \n",
    "        # Logic for Gender Flip Rate\n",
    "        if entry['type'] == 'gender_bias':\n",
    "            conf = metrics.get('gender_conf', 0.5)\n",
    "            cls = metrics.get('gender_class', '')\n",
    "            row['female_prob'] = conf if \"woman\" in cls else 1.0 - conf\n",
    "            \n",
    "            gen_ref = metrics.get('gender_original', 'unknown')\n",
    "            gen_trans = metrics.get('gender_class', 'unknown')\n",
    "            ref_label = \"woman\" if \"woman\" in gen_ref else \"man\"\n",
    "            trans_label = \"woman\" if \"woman\" in gen_trans else \"man\"\n",
    "            row['has_flipped'] = 1 if (ref_label != trans_label) else 0\n",
    "\n",
    "        rows.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    df = df.sort_values(['id', 'seed'])\n",
    "    df['Variation'] = df.groupby('id').cumcount() + 1\n",
    "    df['Variation'] = df['Variation'].astype(str) \n",
    "\n",
    "    # --- CHART 1: CONTEXT BIAS ---\n",
    "    df_ctx = df[df['type'] == 'context_bias']\n",
    "    if not df_ctx.empty:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "        \n",
    "        # SSIM\n",
    "        sns.barplot(data=df_ctx, x=\"id\", y=\"background_ssim\", hue=\"Variation\", ax=axes[0], palette=\"viridis\")\n",
    "        axes[0].set_title(\"Structural Similarity (SSIM)\", fontsize=16, fontweight='bold')\n",
    "        axes[0].set_ylabel(\"Similarity (1.0 = Identical)\")\n",
    "        axes[0].set_xlabel(\"\")\n",
    "        axes[0].set_ylim(0, 1.0)\n",
    "        axes[0].axhline(0.5, color='r', linestyle='--', alpha=0.3)\n",
    "        axes[0].legend(title=\"Run #\", loc='lower right', fontsize=10)\n",
    "\n",
    "        # LPIPS\n",
    "        sns.barplot(data=df_ctx, x=\"id\", y=\"background_lpips\", hue=\"Variation\", ax=axes[1], palette=\"magma_r\")\n",
    "        axes[1].set_title(\"Perceptual Distance (LPIPS)\", fontsize=16, fontweight='bold')\n",
    "        axes[1].set_ylabel(\"Distance (Lower is Better)\")\n",
    "        axes[1].set_xlabel(\"\")\n",
    "        axes[1].set_ylim(0, 0.8)\n",
    "        axes[1].legend(title=\"Run #\", loc='lower right', fontsize=10)\n",
    "        \n",
    "        plt.suptitle(\"Context Bias Analysis\", fontsize=18)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(ANALYSIS_DIR, \"CHART_1_Context_Bias_Dual.png\"), bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "    # --- CHART 2: ATTRIBUTE LEAKAGE ---\n",
    "    df_attr = df[df['type'] == 'attribute_leakage']\n",
    "    if not df_attr.empty:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "        \n",
    "        # Texture (Zebra) - Using Seed directly on X for detail since it's one scenario\n",
    "        df_tex = df_attr[df_attr['id'].str.contains(\"zebra\")]\n",
    "        if not df_tex.empty:\n",
    "            sns.barplot(data=df_tex, x=\"seed\", y=\"texture_conf\", hue=\"texture_class\", ax=axes[0], palette=\"magma\")\n",
    "            axes[0].set_title(\"Texture Entanglement (Zebra)\", fontsize=16)\n",
    "            axes[0].set_ylabel(\"Confidence (0.0 - 1.0)\")\n",
    "            axes[0].set_ylim(0, 1.0)\n",
    "        \n",
    "        # Color (Fish/Flamingo) - Grouped by Variation\n",
    "        df_col = df_attr[~df_attr['id'].str.contains(\"zebra\")]\n",
    "        if not df_col.empty:\n",
    "            sns.barplot(data=df_col, x=\"id\", y=\"avg_color_rgb_R\", hue=\"Variation\", ax=axes[1], palette=\"Reds\")\n",
    "            axes[1].set_title(\"Color Leakage (Red Intensity)\", fontsize=16)\n",
    "            axes[1].set_ylabel(\"Pixel Intensity (0 - 255)\")\n",
    "            axes[1].set_ylim(0, 255)\n",
    "            axes[1].legend(title=\"Run #\")\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(ANALYSIS_DIR, \"CHART_2_Attribute_Leakage.png\"), bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "    # --- CHART 3: SOCIETAL BIAS ---\n",
    "    df_soc = df[df['type'] == 'gender_bias']\n",
    "    if not df_soc.empty:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "        # Avg Probability \n",
    "        avg_probs = df_soc.groupby('id')['female_prob'].mean().sort_values()\n",
    "        custom_palette = sns.color_palette(\"coolwarm\", as_cmap=True)(avg_probs.values)\n",
    "\n",
    "        sns.barplot(\n",
    "            data=df_soc, x=\"id\", y=\"female_prob\", \n",
    "            estimator=np.mean, ci=None, \n",
    "            ax=axes[0], palette=custom_palette\n",
    "        )\n",
    "        \n",
    "        axes[0].set_title(\"A. Average Predicted Gender\", fontsize=16, fontweight='bold')\n",
    "        axes[0].set_ylabel(\"Prob. of being Female (0-1)\", fontsize=14)\n",
    "        axes[0].set_xlabel(\"\")\n",
    "        axes[0].set_ylim(0, 1.0)\n",
    "        axes[0].axhline(0.5, color='gray', linestyle='--', linewidth=2)\n",
    "        \n",
    "        axes[0].axhspan(0.5, 1.0, color='pink', alpha=0.1, label=\"Female Zone\")\n",
    "        axes[0].axhspan(0.0, 0.5, color='lightblue', alpha=0.1, label=\"Male Zone\")\n",
    "        axes[0].text(0.5, 0.9, \"Female Prediction\", ha='center', color='purple', transform=axes[0].transAxes)\n",
    "        axes[0].text(0.5, 0.1, \"Male Prediction\", ha='center', color='navy', transform=axes[0].transAxes)\n",
    "\n",
    "        # Flip Rate \n",
    "        sns.barplot(\n",
    "            data=df_soc, x=\"id\", y=\"has_flipped\", \n",
    "            estimator=lambda x: np.mean(x)*100, \n",
    "            ci=None, \n",
    "            ax=axes[1], palette=\"Reds\"\n",
    "        )\n",
    "        \n",
    "        axes[1].set_title(\"B. Frequency of Gender Flip\", fontsize=16, fontweight='bold')\n",
    "        axes[1].set_ylabel(\"Flip Rate (%)\", fontsize=14)\n",
    "        axes[1].set_xlabel(\"\")\n",
    "        axes[1].set_ylim(0, 100)\n",
    "        \n",
    "        for container in axes[1].containers:\n",
    "            axes[1].bar_label(container, fmt='%.0f%%', padding=5, fontsize=12, fontweight='bold')\n",
    "\n",
    "        plt.suptitle(\"Societal Bias Analysis: Stereotype & Consistency\", fontsize=20)\n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(top=0.88) \n",
    "        \n",
    "        plt.savefig(os.path.join(ANALYSIS_DIR, \"CHART_3_Societal_Bias_Combined.png\"), bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    print(\"Final Report Generated.\")\n",
    "\n",
    "generate_visual_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff4ca74",
   "metadata": {},
   "source": [
    "# 5. Investigation IV: The \"Surgery Autopilot\" (Automation)\n",
    "\n",
    "## Part A: Automated Training Data Generation\n",
    "\n",
    "This section executes an exhaustive **Grid Search** to automatically generate the Ground Truth data required to train the Autopilot Regression Model.\n",
    "\n",
    "### Objective\n",
    "To build a dataset mapping arbitrary semantic shifts (Text Prompts) to their ideal surgical parameters ($\\lambda, \\alpha$).\n",
    "\n",
    "### Methodology: Multi-Objective Optimization\n",
    "For each of the **52 diverse scenarios** (covering Animals, Vehicles, Objects, and Contexts), the system performs a grid search across a pre-defined hyperparameter space to find the configuration that maximizes a weighted quality score.\n",
    "\n",
    "* **Search Space**:\n",
    "    * **Force ($\\lambda$):** $\\{0.8, 1.0, 1.2, 1.4\\}$\n",
    "    * **Sensitivity ($\\alpha$):** $\\{0.10, 0.15, 0.20\\}$\n",
    "* **The \"Golden Score\"**:\n",
    "    $$\\text{Score} = (W_{CLIP} \\cdot \\frac{\\text{CLIP}}{30}) + (W_{SSIM} \\cdot \\text{SSIM})$$\n",
    "    * Where $W_{CLIP}=0.6$ (prioritizing semantic change) and $W_{SSIM}=0.4$ (preserving structure).\n",
    "\n",
    "The resulting best parameters for each scenario are saved to `final_training_dataset/dataset_log.csv` and will serve as the **Target Labels ($Y$)** for the machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc7abcfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Successfully Generated in 0.0 minutes!\n",
      "Data saved in: results/final_training_dataset/dataset_log.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scenario_ID</th>\n",
       "      <th>Optimal_Force</th>\n",
       "      <th>Optimal_Sens</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ani_Bear_Tiger</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.733057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ani_Dog_Cat</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.810182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ani_Fish_Shark</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.763661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ani_Horse_Zebra</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.885776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ani_Bird_Parrot</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.828637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Veh_Car_Firetruck</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.806777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Veh_Boat_Yacht</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.792884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Veh_Bike_Moto</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.811055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Veh_Plane_Bird</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.786461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Veh_Bus_Train</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.835648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Scenario_ID  Optimal_Force  Optimal_Sens     Score\n",
       "0     Ani_Bear_Tiger            1.2          0.10  0.733057\n",
       "1        Ani_Dog_Cat            1.0          0.15  0.810182\n",
       "2     Ani_Fish_Shark            1.0          0.15  0.763661\n",
       "3    Ani_Horse_Zebra            1.0          0.20  0.885776\n",
       "4    Ani_Bird_Parrot            0.8          0.10  0.828637\n",
       "5  Veh_Car_Firetruck            0.8          0.20  0.806777\n",
       "6     Veh_Boat_Yacht            1.2          0.20  0.792884\n",
       "7      Veh_Bike_Moto            0.8          0.10  0.811055\n",
       "8     Veh_Plane_Bird            1.0          0.20  0.786461\n",
       "9      Veh_Bus_Train            1.4          0.15  0.835648"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_dir = \"results/final_training_dataset\"\n",
    "os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "# Grid Search Space\n",
    "search_forces = [0.8, 1.0, 1.2, 1.4]\n",
    "search_sensitivities = [0.10, 0.15, 0.20]\n",
    "\n",
    "# Optimization Weights\n",
    "W_CLIP = 0.6\n",
    "W_SSIM = 0.4\n",
    "\n",
    "# 52 Scenarios covering multiple domains\n",
    "scenarios_to_test = [\n",
    "    {\"id\": \"Ani_Bear_Tiger\", \"prompt\": \"A brown bear walking in a forest\", \"remove\": \"brown bear\", \"inject\": \"tiger\"}, \n",
    "    {\"id\": \"Ani_Dog_Cat\", \"prompt\": \"A golden retriever sitting on a sofa\", \"remove\": \"golden retriever\", \"inject\": \"tabby cat\"}, \n",
    "    {\"id\": \"Ani_Fish_Shark\", \"prompt\": \"A goldfish swimming in a glass bowl\", \"remove\": \"goldfish\", \"inject\": \"shark\"}, \n",
    "    {\"id\": \"Ani_Horse_Zebra\", \"prompt\": \"A brown horse galloping in a field\", \"remove\": \"brown horse\", \"inject\": \"zebra\"}, \n",
    "    {\"id\": \"Ani_Bird_Parrot\", \"prompt\": \"A sparrow sitting on a tree branch\", \"remove\": \"sparrow\", \"inject\": \"colorful parrot\"}, \n",
    "    {\"id\": \"Veh_Car_Firetruck\", \"prompt\": \"A red sedan driving on a highway\", \"remove\": \"red sedan\", \"inject\": \"firetruck\"}, \n",
    "    {\"id\": \"Veh_Boat_Yacht\", \"prompt\": \"A wooden fishing boat in the ocean\", \"remove\": \"wooden fishing boat\", \"inject\": \"luxury yacht\"}, \n",
    "    {\"id\": \"Veh_Bike_Moto\", \"prompt\": \"A bicycle parked against a wall\", \"remove\": \"bicycle\", \"inject\": \"motorcycle\"}, \n",
    "    {\"id\": \"Veh_Bus_Train\", \"prompt\": \"A yellow school bus on the street\", \"remove\": \"yellow school bus\", \"inject\": \"green tram\"}, \n",
    "    {\"id\": \"Obj_Apple_Orange\", \"prompt\": \"A red apple on a wooden table\", \"remove\": \"red apple\", \"inject\": \"orange\"}, \n",
    "    {\"id\": \"Obj_Pizza_Cake\", \"prompt\": \"A pepperoni pizza on a plate\", \"remove\": \"pepperoni pizza\", \"inject\": \"chocolate cake\"}, \n",
    "    {\"id\": \"Obj_Coffee_Beer\", \"prompt\": \"A cup of hot coffee on a desk\", \"remove\": \"cup of hot coffee\", \"inject\": \"glass of beer\"}, \n",
    "    {\"id\": \"Obj_Bottle_Can\", \"prompt\": \"A glass wine bottle on a counter\", \"remove\": \"glass wine bottle\", \"inject\": \"soda can\"}, \n",
    "    {\"id\": \"Obj_Shoe_Boot\", \"prompt\": \"A running shoe on the floor\", \"remove\": \"running shoe\", \"inject\": \"leather boot\"}, \n",
    "    {\"id\": \"Ctx_Corgi_Snow\", \"prompt\": \"A corgi dog running on green grass\", \"remove\": \"green grass\", \"inject\": \"snowy field\"}, \n",
    "    {\"id\": \"Ani_Lion_Cat\", \"prompt\": \"A lion lying in the savanna\", \"remove\": \"lion\", \"inject\": \"house cat\"}, \n",
    "    {\"id\": \"Ani_Wolf_Dog\", \"prompt\": \"A grey wolf standing in the snow\", \"remove\": \"grey wolf\", \"inject\": \"husky dog\"}, \n",
    "    {\"id\": \"Ani_Duck_Swan\", \"prompt\": \"A duck swimming in a pond\", \"remove\": \"duck\", \"inject\": \"white swan\"}, \n",
    "    {\"id\": \"Veh_Truck_Van\", \"prompt\": \"A large truck driving on a road\", \"remove\": \"large truck\", \"inject\": \"delivery van\"}, \n",
    "    {\"id\": \"Veh_Scooter_Bike\", \"prompt\": \"A vespa scooter parked on the street\", \"remove\": \"vespa scooter\", \"inject\": \"bicycle\"}, \n",
    "    {\"id\": \"Obj_Laptop_Book\", \"prompt\": \"A laptop open on a wooden desk\", \"remove\": \"laptop\", \"inject\": \"open book\"}, \n",
    "    {\"id\": \"Obj_Candle_Lamp\", \"prompt\": \"A lit candle on a dark table\", \"remove\": \"lit candle\", \"inject\": \"table lamp\"}, \n",
    "    {\"id\": \"Obj_Burger_Sandwich\", \"prompt\": \"A cheeseburger on a fast food tray\", \"remove\": \"cheeseburger\", \"inject\": \"sandwich\"}, \n",
    "    {\"id\": \"Fash_Hat_Helmet\", \"prompt\": \"A baseball cap on a wooden shelf\", \"remove\": \"baseball cap\", \"inject\": \"bicycle helmet\"}, \n",
    "    {\"id\": \"Fash_Shoe_Sandal\", \"prompt\": \"A running shoe on the floor\", \"remove\": \"running shoe\", \"inject\": \"leather sandal\"}, \n",
    "    {\"id\": \"Fash_Shirt_Jacket\", \"prompt\": \"A folded t-shirt on a bed\", \"remove\": \"folded t-shirt\", \"inject\": \"denim jacket\"}, \n",
    "    {\"id\": \"Fash_Bag_Backpack\", \"prompt\": \"A leather handbag on a table\", \"remove\": \"leather handbag\", \"inject\": \"school backpack\"}, \n",
    "    {\"id\": \"Furn_Chair_Armchair\", \"prompt\": \"A wooden chair in an empty room\", \"remove\": \"wooden chair\", \"inject\": \"red armchair\"}, \n",
    "    {\"id\": \"Furn_Lamp_Plant\", \"prompt\": \"A floor lamp standing in a corner\", \"remove\": \"floor lamp\", \"inject\": \"potted plant\"}, \n",
    "    {\"id\": \"Furn_Clock_Painting\", \"prompt\": \"A round wall clock hanging on a wall\", \"remove\": \"round wall clock\", \"inject\": \"framed painting\"}, \n",
    "    {\"id\": \"Furn_Pillow_Cushion\", \"prompt\": \"A white pillow on a bed\", \"remove\": \"white pillow\", \"inject\": \"decorative cushion\"}, \n",
    "    {\"id\": \"Food_Banana_Cucumber\", \"prompt\": \"A yellow banana on a white plate\", \"remove\": \"yellow banana\", \"inject\": \"green cucumber\"}, \n",
    "    {\"id\": \"Food_Donut_Bagel\", \"prompt\": \"A pink donut with sprinkles\", \"remove\": \"pink donut\", \"inject\": \"plain bagel\"}, \n",
    "    {\"id\": \"Food_Mushroom_Flower\", \"prompt\": \"A red mushroom growing in grass\", \"remove\": \"red mushroom\", \"inject\": \"red tulip\"}, \n",
    "    {\"id\": \"Food_Egg_Ball\", \"prompt\": \"A white egg sitting on a table\", \"remove\": \"white egg\", \"inject\": \"ping pong ball\"}, \n",
    "    {\"id\": \"Food_Burger_Taco\", \"prompt\": \"A cheeseburger on a fast food tray\", \"remove\": \"cheeseburger\", \"inject\": \"mexican taco\"}, \n",
    "    {\"id\": \"Veh_Tractor_Tank\", \"prompt\": \"A green tractor in a field\", \"remove\": \"green tractor\", \"inject\": \"military tank\"}, \n",
    "    {\"id\": \"Veh_Helicopter_Drone\", \"prompt\": \"A helicopter flying in the sky\", \"remove\": \"helicopter\", \"inject\": \"quadcopter drone\"}, \n",
    "    {\"id\": \"Veh_Scooter_Skateboard\", \"prompt\": \"A scooter parked on pavement\", \"remove\": \"scooter\", \"inject\": \"skateboard\"}, \n",
    "    {\"id\": \"OOD_Fish_Bird\", \"prompt\": \"A goldfish swimming in a bowl\", \"remove\": \"goldfish\", \"inject\": \"small bird\"}, \n",
    "    {\"id\": \"OOD_Car_Boat\", \"prompt\": \"A car parked in a garage\", \"remove\": \"car\", \"inject\": \"small boat\"}, \n",
    "    {\"id\": \"OOD_Tree_Lamp\", \"prompt\": \"A large oak tree in a park\", \"remove\": \"large oak tree\", \"inject\": \"giant street lamp\"}, \n",
    "    {\"id\": \"Sport_Soccer_Basket\", \"prompt\": \"A soccer ball on a grass field\", \"remove\": \"soccer ball\", \"inject\": \"basketball\"}, \n",
    "    {\"id\": \"Sport_Tennis_Baseball\", \"prompt\": \"A tennis ball lying on the court\", \"remove\": \"tennis ball\", \"inject\": \"baseball\"},\n",
    "    {\"id\": \"Toy_Teddy_Robot\", \"prompt\": \"A brown teddy bear sitting on a bed\", \"remove\": \"brown teddy bear\", \"inject\": \"toy robot\"}, \n",
    "    {\"id\": \"Music_Guitar_Electric\", \"prompt\": \"An acoustic guitar leaning against a wall\", \"remove\": \"acoustic guitar\", \"inject\": \"electric guitar\"}, \n",
    "    {\"id\": \"Music_Violin_Cello\", \"prompt\": \"A violin resting on a chair\", \"remove\": \"violin\", \"inject\": \"cello\"}, \n",
    "    {\"id\": \"Kit_Mug_Glass\", \"prompt\": \"A white ceramic mug on a table\", \"remove\": \"white ceramic mug\", \"inject\": \"transparent glass cup\"}, \n",
    "    {\"id\": \"Kit_Bowl_Pot\", \"prompt\": \"A wooden bowl on a kitchen counter\", \"remove\": \"wooden bowl\", \"inject\": \"cooking pot\"}, \n",
    "    {\"id\": \"Nat_Rock_Bush\", \"prompt\": \"A large gray rock in a garden\", \"remove\": \"large gray rock\", \"inject\": \"green bush\"}, \n",
    "    {\"id\": \"Nat_Mushroom_Stump\", \"prompt\": \"A red mushroom growing in the forest\", \"remove\": \"red mushroom\", \"inject\": \"tree stump\"}, \n",
    "    {\"id\": \"Acc_Wallet_Phone\", \"prompt\": \"A leather wallet on a table\", \"remove\": \"leather wallet\", \"inject\": \"smartphone\"}\n",
    "]\n",
    "\n",
    "dataset_rows = []\n",
    "csv_path = os.path.join(dataset_dir, \"dataset_log.csv\")\n",
    "\n",
    "# Resume capability\n",
    "if os.path.exists(csv_path):\n",
    "    print(\"Partial dataset found, loading existing data...\")\n",
    "    dataset_rows = pd.read_csv(csv_path).to_dict('records')\n",
    "    completed_ids = [row['Scenario_ID'] for row in dataset_rows]\n",
    "else:\n",
    "    completed_ids = []\n",
    "\n",
    "print(f\"Starting Dataset Generation: {len(scenarios_to_test)} Total Scenarios\")\n",
    "print(f\"Completed until now: {len(completed_ids)}\")\n",
    "\n",
    "if 'surgeon' in locals() and 'evaluator' in locals():\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for idx, test in enumerate(scenarios_to_test):\n",
    "        if test['id'] in completed_ids:\n",
    "            continue \n",
    "            \n",
    "        print(f\"\\n[{idx+1}/{len(scenarios_to_test)}] Processing: {test['id']}...\")\n",
    "        \n",
    "        scen_dir = os.path.join(dataset_dir, test['id'])\n",
    "        os.makedirs(scen_dir, exist_ok=True)\n",
    "        \n",
    "        # 1. Reference Image\n",
    "        gen = torch.Generator(\"cpu\").manual_seed(42)\n",
    "        surgeon.concepts_to_erase = []\n",
    "        img_ref = surgeon([test['prompt']], img_size=512, n_steps=30, n_imgs=1, \n",
    "                          show_alpha=False, generator=gen, replace_with=None)[0][0]\n",
    "        img_ref.save(os.path.join(scen_dir, \"reference.png\"))\n",
    "        \n",
    "        # 2. Grid Search Loop\n",
    "        best_run = {\"score\": -1, \"config\": None, \"img\": None}\n",
    "        \n",
    "        combinations = list(itertools.product(search_forces, search_sensitivities))\n",
    "        \n",
    "        for force, sens in combinations:\n",
    "            surgeon.params['lambda'] = force\n",
    "            surgeon.params['alpha_threshold'] = sens\n",
    "            surgeon.concepts_to_erase = [test['remove']]\n",
    "            \n",
    "            gen.manual_seed(42)\n",
    "            img_cand = surgeon([test['prompt']], img_size=512, n_steps=30, n_imgs=1, \n",
    "                               show_alpha=False, generator=gen, replace_with=test['inject'])[0][0]\n",
    "            \n",
    "            # Calculate Scores\n",
    "            clip = evaluator.get_clip_score_single(img_cand, f\"a photo of a {test['inject']}\")\n",
    "            ssim = evaluator.get_ssim_score(img_ref, img_cand)\n",
    "\n",
    "            # Optimization Formula\n",
    "            score = (W_CLIP * (clip/30.0)) + (W_SSIM * ssim)\n",
    "            \n",
    "            if score > best_run[\"score\"]:\n",
    "                best_run[\"score\"] = score\n",
    "                best_run[\"config\"] = {\"F\": force, \"S\": sens, \"CLIP\": clip, \"SSIM\": ssim}\n",
    "                best_run[\"img\"] = img_cand\n",
    "        \n",
    "        # 3. Save Best Result\n",
    "        cfg = best_run[\"config\"]\n",
    "        print(f\"   Best: F={cfg['F']}, S={cfg['S']} (Score: {best_run['score']:.3f})\")\n",
    "        \n",
    "        best_name = f\"best_F{cfg['F']}_S{cfg['S']}.png\"\n",
    "        best_run[\"img\"].save(os.path.join(scen_dir, best_name))\n",
    "        \n",
    "        new_row = {\n",
    "            \"Scenario_ID\": test['id'],\n",
    "            \"Original_Object\": test['remove'],\n",
    "            \"Target_Object\": test['inject'],\n",
    "            \"Prompt\": test['prompt'],\n",
    "            \"Optimal_Force\": cfg['F'],\n",
    "            \"Optimal_Sens\": cfg['S'],\n",
    "            \"Score\": best_run['score'],\n",
    "            \"CLIP_Val\": cfg['CLIP'],\n",
    "            \"SSIM_Val\": cfg['SSIM'],\n",
    "            \"Path_Ref\": os.path.join(scen_dir, \"reference.png\"),\n",
    "            \"Path_Best\": os.path.join(scen_dir, best_name)\n",
    "        }\n",
    "        dataset_rows.append(new_row)\n",
    "        \n",
    "        # Save progress incrementally\n",
    "        pd.DataFrame(dataset_rows).to_csv(csv_path, index=False)\n",
    "    \n",
    "    elapsed = (time.time() - start_time) / 60\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    print(f\"\\nDataset Successfully Generated in {elapsed:.1f} minutes!\")\n",
    "    print(f\"Data saved in: {csv_path}\")\n",
    "    \n",
    "    display(pd.DataFrame(dataset_rows)[['Scenario_ID', 'Optimal_Force', 'Optimal_Sens', 'Score']].head(10))\n",
    "\n",
    "else:\n",
    "    print(\"Error: Load 'surgeon' and 'evaluator' before running.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f5af26",
   "metadata": {},
   "source": [
    "## Part B: Training the \"Surgery Autopilot\" (Machine Learning)\n",
    "\n",
    "This cell trains the baseline Multi-Output Regression model designed to automatically predict the optimal surgical parameters ($\\lambda$ and $\\alpha$) based on the semantic shift defined by the input text.\n",
    "\n",
    "### Feature Extraction and Data Preparation\n",
    "- **Feature Set ($\\mathbf{X}$):** Textual concepts are processed by **CLIP (ViT-B/32)** to generate high-dimensional text embeddings (512-dim), serving as the input features. The input text is formatted as `{Original_Object} -> change to {Target_Object}`.\n",
    "- **Target Variables ($\\mathbf{Y}$):** The targets are the empirically determined optimal values found in the previous step: $[\\text{Optimal\\_Force}, \\text{Optimal\\_Sens}]$.\n",
    "- **Data Split:** The dataset is split into **80% Training** and **20% Test** sets for robust validation.\n",
    "\n",
    "### Model Specification\n",
    "- **Algorithm:** We employ a **Random Forest Regressor** (wrapped in a `MultiOutputRegressor`) with 100 estimators. Random Forests are chosen as the baseline for their robustness to overfitting on small datasets and ability to capture non-linear relationships without extensive tuning.\n",
    "- **Evaluation Metrics:**\n",
    "    - **Mean Absolute Error (MAE)**: Provides the average deviation (error margin) in predicting the Force and Sensitivity values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "371c1a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Trained!\n",
      "\n",
      "Test Set Results:\n",
      "   Mean Absolute Error (MAE): 0.1043\n",
      "   (It means that it misses the Force/Sens by about +/- 0.1043)\n",
      "\n",
      "Real-World Comparison (Top 5 of the test set):\n",
      "Real (F, S)          | Predicted (F, S)     | Error     \n",
      "------------------------------------------------------------\n",
      "[1.20, 0.10]      | [1.09, 0.14]      | 0.071\n",
      "[0.80, 0.15]      | [1.04, 0.16]      | 0.122\n",
      "[0.80, 0.20]      | [0.99, 0.15]      | 0.119\n",
      "[1.20, 0.20]      | [1.00, 0.14]      | 0.126\n",
      "[0.80, 0.10]      | [0.94, 0.15]      | 0.094\n",
      "\n",
      "Model saved in: results/models/surgery_autopilot_model.pkl\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"results/final_training_dataset/clean_dataset_for_automation.csv\"\n",
    "\n",
    "# Output Directory\n",
    "models_dir = \"results/models\"\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "model_save_path = os.path.join(models_dir, \"surgery_autopilot_model.pkl\")\n",
    "\n",
    "print(\"Start of 'Surgery Autopilot' training...\")\n",
    "\n",
    "if not os.path.exists(dataset_path):\n",
    "    dataset_path = \"final_training_dataset/dataset_log.csv\"\n",
    "    \n",
    "df = pd.read_csv(dataset_path)\n",
    "print(f\"Loaded dataset: {len(df)} examples.\")\n",
    "\n",
    "print(\"Loading CLIP for Feature Extraction...\")\n",
    "model_id = \"openai/clip-vit-base-patch32\"\n",
    "processor = CLIPProcessor.from_pretrained(model_id)\n",
    "clip_model = CLIPModel.from_pretrained(model_id)\n",
    "\n",
    "def extract_features(prompts, targets):\n",
    "    inputs = processor(text=prompts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        text_features = clip_model.get_text_features(**inputs)\n",
    "    return text_features.numpy()\n",
    "\n",
    "print(\"   > Extracting Embeddings from the prompts...\")\n",
    "\n",
    "combined_text = [f\"{row['Prompt']} -> change to {row['Target_Object']}\" for _, row in df.iterrows()]\n",
    "\n",
    "X = extract_features(combined_text, None) \n",
    "y = df[['Optimal_Force', 'Optimal_Sens']].values \n",
    "\n",
    "# 80/20 Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"   > Training Set: {len(X_train)} | Test Set: {len(X_test)}\")\n",
    "\n",
    "# Train Random Forest\n",
    "regr = MultiOutputRegressor(RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "regr.fit(X_train, y_train)\n",
    "\n",
    "clear_output(wait=True)\n",
    "\n",
    "print(\"Model Trained!\")\n",
    "\n",
    "# Evaluate\n",
    "y_pred = regr.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nTest Set Results:\")\n",
    "print(f\"   Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"   (It means that it misses the Force/Sens by about +/- {mae:.4f})\")\n",
    "\n",
    "print(\"\\nReal-World Comparison (Top 5 of the test set):\")\n",
    "print(f\"{'Real (F, S)':<20} | {'Predicted (F, S)':<20} | {'Error':<10}\")\n",
    "print(\"-\" * 60)\n",
    "for i in range(min(5, len(y_test))):\n",
    "    real = y_test[i]\n",
    "    pred = y_pred[i]\n",
    "    diff = np.abs(real - pred)\n",
    "    print(f\"[{real[0]:.2f}, {real[1]:.2f}]      | [{pred[0]:.2f}, {pred[1]:.2f}]      | {np.mean(diff):.3f}\")\n",
    "\n",
    "# Save Model\n",
    "with open(model_save_path, 'wb') as f:\n",
    "    pickle.dump(regr, f)\n",
    "print(f\"\\nModel saved in: {model_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78e8041",
   "metadata": {},
   "source": [
    "## Part C: Advanced Automation with Deep Learning (SurgeryNet Training)\n",
    "\n",
    "In this step, we train a custom Deep Learning model to challenge the baseline Machine Learning approach. We introduce **SurgeryNet**, a neural network designed to capture the non-linear relationships between semantic embeddings and surgical parameters.\n",
    "\n",
    "### Architecture: SurgeryNet\n",
    "We designed a lightweight **Multi-Layer Perceptron (MLP)** optimized for regression on high-dimensional vectors:\n",
    "* **Input**: 512-dim CLIP vector.\n",
    "* **Hidden Layers**: 3 fully connected layers ($256 \\to 128 \\to 64$) equipped with:\n",
    "    * **Batch Normalization**: To stabilize learning.\n",
    "    * **ReLU Activation**: To model non-linearities.\n",
    "    * **Dropout (0.2)**: To prevent overfitting on the limited dataset size.\n",
    "* **Output**: 2 continuous values (Force $\\lambda$, Sensitivity $\\alpha$).\n",
    "\n",
    "### Training Protocol\n",
    "We train the network using the **Mean Squared Error (MSE)** loss function and the **Adam** optimizer. Based on our previous optimization analysis, we set the training duration to **600 epochs**—the empirically determined \"early stopping\" point where the model generalizes best without memorizing noise.\n",
    "\n",
    "The trained weights are saved to `models/surgery_net.pth` for the upcoming comparative evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c358d70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Deep Learning Model (SurgeryNet)...\n",
      "   Epoch 50/600 | Loss: 0.0120\n",
      "   Epoch 100/600 | Loss: 0.0075\n",
      "   Epoch 150/600 | Loss: 0.0070\n",
      "   Epoch 200/600 | Loss: 0.0046\n",
      "   Epoch 250/600 | Loss: 0.0031\n",
      "   Epoch 300/600 | Loss: 0.0035\n",
      "   Epoch 350/600 | Loss: 0.0034\n",
      "   Epoch 400/600 | Loss: 0.0033\n",
      "   Epoch 450/600 | Loss: 0.0023\n",
      "   Epoch 500/600 | Loss: 0.0027\n",
      "   Epoch 550/600 | Loss: 0.0027\n",
      "   Epoch 600/600 | Loss: 0.0025\n",
      "\n",
      "SurgeryNet Model saved to: results/models/surgery_net.pth\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# --- SURGERY NET ARCHITECTURE ---\n",
    "class SurgeryNet(nn.Module):\n",
    "    def __init__(self, input_dim=512, output_dim=2):\n",
    "        super(SurgeryNet, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(64, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "def train_dl_model(X_train, y_train, epochs=200, batch_size=8, lr=0.001):\n",
    "    set_seed(42) \n",
    "    \n",
    "    print(\"Training Deep Learning Model (SurgeryNet)...\")\n",
    "    \n",
    "    X_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_t = torch.tensor(y_train, dtype=torch.float32)\n",
    "    \n",
    "    dataset = TensorDataset(X_t, y_t)\n",
    "    \n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    \n",
    "    model = SurgeryNet()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    loss_history = []\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        batch_count = 0\n",
    "        for batch_X, batch_y in loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            batch_count += 1\n",
    "            \n",
    "        if batch_count > 0:\n",
    "            loss_history.append(epoch_loss / batch_count)\n",
    "        \n",
    "        if (epoch+1) % 50 == 0:\n",
    "            print(f\"   Epoch {epoch+1}/{epochs} | Loss: {loss_history[-1]:.4f}\")\n",
    "            \n",
    "    return model, loss_history\n",
    "\n",
    "if 'X_train' in locals() and 'X_test' in locals():\n",
    "    \n",
    "    # 1. Train\n",
    "    dl_model, history = train_dl_model(X_train, y_train, epochs=600)\n",
    "    \n",
    "    # 2. Save Model\n",
    "    models_dir = \"results/models\"\n",
    "    os.makedirs(models_dir, exist_ok=True)\n",
    "    save_path = os.path.join(models_dir, \"surgery_net.pth\")\n",
    "    torch.save(dl_model.state_dict(), save_path)\n",
    "\n",
    "    print(f\"\\nSurgeryNet Model saved to: {save_path}\")\n",
    "\n",
    "else:\n",
    "    print(\"Error: Run the previous training cell to define X_train, X_test first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fa357c",
   "metadata": {},
   "source": [
    "## Part D: Model Selection & Comparative Analysis (The \"Showdown\")\n",
    "\n",
    "This cell executes the final head-to-head comparison between the classical **Machine Learning** approach (Random Forest) and the proposed **Deep Learning** architecture (SurgeryNet).\n",
    "\n",
    "### Objective\n",
    "To empirically determine which model better captures the non-linear relationship between the semantic meaning of a prompt (CLIP Embedding) and the optimal surgical parameters required to modify it.\n",
    "\n",
    "### Evaluation Metrics\n",
    "Both models are evaluated on the held-out **Test Set** (20% of data) using:\n",
    "1.  **Mean Absolute Error (MAE):** Measures the average \"distance\" between the predicted parameters and the ground truth. A lower MAE indicates higher precision.\n",
    "2.  **Training Stability Analysis:** We plot the **Loss Curve** of the neural network to verify convergence and ensure that 600 epochs represent the optimal training duration without overfitting.\n",
    "\n",
    "### Visualization\n",
    "* **Loss Dynamics:** A line chart showing the Neural Network's learning progress.\n",
    "* **The Verdict:** A bar chart comparing the final MAE of both models to visually declare the winner of the automation challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62774d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "      FINAL SHOWDOWN: ML vs DL\n",
      "========================================\n",
      "Random Forest MAE: 0.1043\n",
      "Deep Learning MAE: 0.0918\n",
      "\n",
      "WINNER: Neural Network (DL)\n",
      "Difference: 0.0125\n",
      "Note: The network generalized better.\n",
      "\n",
      "Comparison plot saved successfully to: results/models/ML_vs_DL_Comparison.png\n"
     ]
    }
   ],
   "source": [
    "if 'X_train' in locals() and 'X_test' in locals():\n",
    "\n",
    "    # 3. Evaluate\n",
    "    dl_model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "        y_pred_dl = dl_model(X_test_tensor).numpy()\n",
    "    \n",
    "    mae_dl = mean_absolute_error(y_test, y_pred_dl)\n",
    "\n",
    "    y_pred_rf = regr.predict(X_test)\n",
    "    mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"      FINAL SHOWDOWN: ML vs DL\")\n",
    "    print(\"=\"*40)\n",
    "    print(f\"Random Forest MAE: {mae_rf:.4f}\")\n",
    "    print(f\"Deep Learning MAE: {mae_dl:.4f}\")\n",
    "    \n",
    "    if mae_rf < mae_dl:\n",
    "        winner = \"Random Forest (ML)\"\n",
    "        diff = mae_dl - mae_rf\n",
    "        reason = \"Small dataset favors classical ML.\"\n",
    "    else:\n",
    "        winner = \"Neural Network (DL)\"\n",
    "        diff = mae_rf - mae_dl\n",
    "        reason = \"The network generalized better.\"\n",
    "        \n",
    "    print(f\"\\nWINNER: {winner}\")\n",
    "    print(f\"Difference: {diff:.4f}\")\n",
    "    print(f\"Note: {reason}\")\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    if history:\n",
    "        axes[0].plot(history, label=\"Training Loss\", color='purple')\n",
    "        axes[0].set_title(\"DL Training Curve (SurgeryNet)\")\n",
    "        axes[0].set_xlabel(\"Epochs\")\n",
    "        axes[0].set_ylabel(\"MSE Loss\")\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    models_names = ['Random Forest (ML)', 'SurgeryNet (DL)']\n",
    "    maes = [mae_rf, mae_dl]\n",
    "    colors = ['#4e79a7', '#e15759'] \n",
    "    \n",
    "    bars = axes[1].bar(models_names, maes, color=colors, alpha=0.8)\n",
    "    axes[1].set_title(f\"Performance Comparison (Lower MAE is Better)\")\n",
    "    axes[1].set_ylabel(\"Mean Absolute Error\")\n",
    "    axes[1].set_ylim(0, max(maes) * 1.3)\n",
    "    \n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        axes[1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.4f}', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plot_filename = \"results/models/ML_vs_DL_Comparison.png\"\n",
    "    plt.savefig(plot_filename)\n",
    "    plt.close() \n",
    "    \n",
    "    print(f\"\\nComparison plot saved successfully to: {plot_filename}\")\n",
    "\n",
    "else:\n",
    "    print(\"Error: Run the previous training cell to define X_train, X_test first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e3807a",
   "metadata": {},
   "source": [
    "## Part E: Automated Comparative Testing (ML vs. DL)\n",
    "\n",
    "This cell integrates both trained models—the classical **Random Forest** (ML) and the neural **SurgeryNet** (DL)—into the live generation pipeline to perform a side-by-side comparative evaluation on unseen prompts.\n",
    "\n",
    "### Workflow: The \"Head-to-Head\" Challenge\n",
    "For every test scenario, the system executes two parallel inference tracks:\n",
    "\n",
    "1.  **Track A (Classical ML)**:\n",
    "    * **Input:** CLIP embedding (Numpy array).\n",
    "    * **Model:** Random Forest Regressor.\n",
    "    * **Output:** Predicted parameters $(\\lambda_{ML}, \\alpha_{ML})$.\n",
    "    * **Result:** Image generated with ML parameters saved to `model_testing/ML/`.\n",
    "\n",
    "2.  **Track B (Deep Learning)**:\n",
    "    * **Input:** CLIP embedding (PyTorch Tensor).\n",
    "    * **Model:** SurgeryNet (MLP).\n",
    "    * **Output:** Predicted parameters $(\\lambda_{DL}, \\alpha_{DL})$.\n",
    "    * **Result:** Image generated with DL parameters saved to `model_testing/DL/`.\n",
    "\n",
    "### Real-World Test Cases\n",
    "We test the models on a diverse set of conceptual swaps (semantic, contextual, object-level) to validate generalization:\n",
    "\n",
    "| Domain | Swap Example | Type |\n",
    "| :--- | :--- | :--- |\n",
    "| **Context** | `sea` $\\rightarrow$ `sand dunes` | Background Injection |\n",
    "| **Object** | `dog` $\\rightarrow$ `cat` | Object-to-Object |\n",
    "| **Abstract** | `lightbulb` $\\rightarrow$ `firefly` | Conceptual/Metaphorical |\n",
    "| **Profession** | `doctor` $\\rightarrow$ `nurse` | Gender/Role Bias Test |\n",
    "\n",
    "The output images allow for a direct visual comparison: **which model understood the semantic intent better?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bcc760b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All tests completed!\n",
      "ML Results: results/model_testing/ML/\n",
      "DL Results: results/model_testing/DL/\n"
     ]
    }
   ],
   "source": [
    "BASE_TEST_DIR = \"results/model_testing\"\n",
    "ML_DIR = os.path.join(BASE_TEST_DIR, \"ML\")\n",
    "DL_DIR = os.path.join(BASE_TEST_DIR, \"DL\")\n",
    "\n",
    "os.makedirs(ML_DIR, exist_ok=True)\n",
    "os.makedirs(DL_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Output folders set:\\n  -> {ML_DIR}/\\n  -> {DL_DIR}/\")\n",
    "\n",
    "MODELS_DIR = \"results/models\"\n",
    "ml_path = os.path.join(MODELS_DIR, \"surgery_autopilot_model.pkl\")\n",
    "dl_path = os.path.join(MODELS_DIR, \"surgery_net.pth\")\n",
    "\n",
    "# Re-define DL Architecture for loadin\n",
    "class SurgeryNet(nn.Module):\n",
    "    def __init__(self, input_dim=512, output_dim=2):\n",
    "        super(SurgeryNet, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(0.2),\n",
    "            nn.Linear(256, 128), nn.BatchNorm1d(128), nn.ReLU(), nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64), nn.ReLU(),\n",
    "            nn.Linear(64, output_dim)\n",
    "        )\n",
    "    def forward(self, x): return self.model(x)\n",
    "\n",
    "if os.path.exists(ml_path) and os.path.exists(dl_path) and 'surgeon' in locals():\n",
    "    print(\"Loading Models...\")\n",
    "    \n",
    "    # Load ML\n",
    "    with open(ml_path, 'rb') as f:\n",
    "        ml_model = pickle.load(f)\n",
    "    \n",
    "    # Load DL\n",
    "    dl_model = SurgeryNet()\n",
    "    dl_model.load_state_dict(torch.load(dl_path))\n",
    "    dl_model.eval()\n",
    "        \n",
    "    # Load CLIP\n",
    "    if 'processor' not in locals():\n",
    "        model_id = \"openai/clip-vit-base-patch32\"\n",
    "        processor = CLIPProcessor.from_pretrained(model_id)\n",
    "        clip_model = CLIPModel.from_pretrained(model_id)\n",
    "    \n",
    "    print(\"Models Loaded Successfully.\")\n",
    "\n",
    "    def run_comparative_surgery(prompt, remove_obj, target_obj, seed=42):\n",
    "        print(f\"\\n--- TEST: '{remove_obj}' -> '{target_obj}' ---\")\n",
    "\n",
    "        # Feature Extraction\n",
    "        text_input = f\"{prompt} -> change to {target_obj}\"\n",
    "        inputs = processor(text=[text_input], return_tensors=\"pt\", padding=True)\n",
    "        with torch.no_grad():\n",
    "            emb_tensor = clip_model.get_text_features(**inputs) \n",
    "            emb_numpy = emb_tensor.numpy()                      \n",
    "        \n",
    "        # Define Tests\n",
    "        tests = [\n",
    "            (\"ML\", ml_model, emb_numpy, ML_DIR),\n",
    "            (\"DL\", dl_model, emb_tensor, DL_DIR)\n",
    "        ]\n",
    "        \n",
    "        for model_name, model, input_data, save_dir in tests:\n",
    "            # Predict\n",
    "            if model_name == \"ML\":\n",
    "                pred = model.predict(input_data)[0]\n",
    "            else: \n",
    "                with torch.no_grad():\n",
    "                    pred = model(input_data).numpy()[0]\n",
    "            \n",
    "            # Post-Process Prediction\n",
    "            pred_force = max(0.6, min(round(float(pred[0]), 2), 1.5))\n",
    "            pred_sens = max(0.05, min(round(float(pred[1]), 2), 0.30))\n",
    "            \n",
    "            print(f\"   [{model_name}] Suggestion: Force={pred_force}, Sens={pred_sens}\")\n",
    "            \n",
    "            # Generate\n",
    "            gen = torch.Generator(\"cpu\").manual_seed(seed)\n",
    "            \n",
    "            # Original Reference\n",
    "            surgeon.concepts_to_erase = []\n",
    "            img_ref = surgeon([prompt], img_size=512, n_steps=30, n_imgs=1, \n",
    "                              show_alpha=False, generator=gen, replace_with=None)[0][0]\n",
    "\n",
    "            # Surgery\n",
    "            gen.manual_seed(seed) \n",
    "            surgeon.params['lambda'] = pred_force\n",
    "            surgeon.params['alpha_threshold'] = pred_sens\n",
    "            surgeon.concepts_to_erase = [remove_obj]\n",
    "            \n",
    "            img_res = surgeon([prompt], img_size=512, n_steps=30, n_imgs=1, \n",
    "                              show_alpha=False, generator=gen, replace_with=target_obj)[0][0]\n",
    "            \n",
    "            # Save Comparison\n",
    "            fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "            ax[0].imshow(img_ref); ax[0].set_title(\"Original\"); ax[0].axis('off')\n",
    "            ax[1].imshow(img_res); ax[1].set_title(f\"{model_name} Result\\n(F={pred_force}, S={pred_sens})\"); ax[1].axis('off')\n",
    "            \n",
    "            clean_prompt = prompt.replace(\" \", \"_\")[:30]\n",
    "            clean_target = target_obj.replace(\" \", \"_\")\n",
    "            filename = f\"{model_name}_{clean_prompt}_TO_{clean_target}.png\"\n",
    "            save_path = os.path.join(save_dir, filename)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(save_path, bbox_inches='tight')\n",
    "            plt.close(fig)\n",
    "\n",
    "    # Run Scenarios \n",
    "    test_cases = [\n",
    "        (\"A red boat in the sea\", \"sea\", \"sand dunes\"),\n",
    "        (\"A goldfish in the ocean\", \"ocean\", \"forest\"),\n",
    "        (\"A lamp on the table\", \"lamp\", \"plant\"),\n",
    "        (\"A red mushroom in the green grass\", \"mushroom\", \"rose\"),\n",
    "        (\"A sofa in a living room\", \"sofa\", \"armchair\"),\n",
    "        (\"A doctor at work\", \"doctor\", \"nurse\"),\n",
    "        (\"A lightbulb glowing in the dark\", \"lightbulb\", \"firefly\"),\n",
    "        (\"A dog on the sofa\", \"dog\", \"cat\")\n",
    "    ]\n",
    "\n",
    "    for p, rem, inj in test_cases:\n",
    "        run_comparative_surgery(p, rem, inj)\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    print(f\"\\nAll tests completed!\")\n",
    "    print(f\"ML Results: {ML_DIR}/\")\n",
    "    print(f\"DL Results: {DL_DIR}/\")\n",
    "\n",
    "else:\n",
    "    print(\"Error: Models not found in 'models/' directory or Surgeon not loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94112c17",
   "metadata": {},
   "source": [
    "# 6. Live Demonstration\n",
    "\n",
    "This final cell launches a **Gradio Web Interface** to provide an interactive playground for the *Semantic Surgery* framework. It serves as the ultimate validation tool, allowing users to experience the difference between manual control and automated prediction in real-time.\n",
    "\n",
    "### The Interface Structure\n",
    "The demo is divided into three distinct tabs, representing the evolution of the project:\n",
    "\n",
    "1.  **Tab 1: Manual Control**\n",
    "    * Allows the user to manually adjust **Force** and **Sensitivity** sliders.\n",
    "    * *Purpose:* To understand the mechanics \"under the hood\" and experience the difficulty of finding the right parameters by hand.\n",
    "\n",
    "2.  **Tab 2: ML Autopilot (Random Forest)**\n",
    "    * The user inputs a semantic shift (e.g., \"Apple\" $\\to$ \"Orange\").\n",
    "    * The system extracts CLIP embeddings and queries the trained **Random Forest** to predict the parameters.\n",
    "    * *Purpose:* To test the baseline automation performance.\n",
    "\n",
    "3.  **Tab 3: DL Autopilot (Neural Network)**\n",
    "    * The user interacts with the advanced **SurgeryNet**.\n",
    "    * *Purpose:* To demonstrate the superior nuance and adaptability of the Deep Learning model trained in the previous step.\n",
    "\n",
    "### Technical Note\n",
    "For every request, the system generates two images side-by-side (Original vs. Modified) using **synchronized random seeds**. This mathematically guarantees that any difference in the image is strictly due to the semantic vector injection, providing immediate visual proof of the surgery's precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbfd59a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Demo...\n",
      "All Models Loaded Successfully (ML & DL).\n",
      "* Running on local URL:  http://127.0.0.1:7860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/Users/giulia/Documents/Università/Advanced Machine Learning/Final Project/Semantic Surgery/src/utils.py:182: FutureWarning: Accessing config attribute `in_channels` directly via 'UNet2DConditionModel' object attribute is deprecated. Please access 'in_channels' over 'UNet2DConditionModel's config object instead, e.g. 'unet.config.in_channels'.\n",
      "  (batch_size, self.unet.in_channels, img_size // 8, img_size // 8),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96505fb7d1344779b0fb633a5e75e270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/15 19:42:44 [W] [service.go:132] login to server failed: dial tcp 44.237.78.176:7000: i/o timeout\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "616f8f5da1e54bcd926e070e39110ab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- 1. SETUP AND LOADING MODELS ---\n",
    "MODELS_DIR = \"results/models\"\n",
    "ML_PATH = os.path.join(MODELS_DIR, \"surgery_autopilot_model.pkl\")\n",
    "DL_PATH = os.path.join(MODELS_DIR, \"surgery_net.pth\")\n",
    "\n",
    "print(\"Initializing Demo...\")\n",
    "\n",
    "# A. Definition of DL Class (Required for loading weights)\n",
    "class SurgeryNet(nn.Module):\n",
    "    def __init__(self, input_dim=512, output_dim=2):\n",
    "        super(SurgeryNet, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(0.2),\n",
    "            nn.Linear(256, 128), nn.BatchNorm1d(128), nn.ReLU(), nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64), nn.ReLU(),\n",
    "            nn.Linear(64, output_dim)\n",
    "        )\n",
    "    def forward(self, x): return self.model(x)\n",
    "\n",
    "# B. Loading Models (Error Handling)\n",
    "models_ready = True\n",
    "try:\n",
    "    with open(ML_PATH, 'rb') as f:\n",
    "        ml_model = pickle.load(f)\n",
    "    \n",
    "    # Load DL (Neural Network)\n",
    "    dl_model = SurgeryNet()\n",
    "    dl_model.load_state_dict(torch.load(DL_PATH))\n",
    "    dl_model.eval() \n",
    "    \n",
    "    # Load CLIP\n",
    "    if 'clip_model' not in globals():\n",
    "        from transformers import CLIPProcessor, CLIPModel\n",
    "        processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        \n",
    "    print(\"All Models Loaded Successfully (ML & DL).\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not load models. Ensure training cells are run. Error: {e}\")\n",
    "    models_ready = False\n",
    "\n",
    "def get_clip_features(text):\n",
    "    inputs = processor(text=[text], return_tensors=\"pt\", padding=True)\n",
    "    with torch.no_grad():\n",
    "        emb = clip_model.get_text_features(**inputs)\n",
    "    return emb\n",
    "\n",
    "def core_generation(prompt, remove, inject, force, sens, seed):\n",
    "    if 'surgeon' not in globals(): return None, None\n",
    "    \n",
    "    # 1. Original (Reference)\n",
    "    gen_orig = torch.Generator(\"cpu\").manual_seed(int(seed))\n",
    "    surgeon.concepts_to_erase = []\n",
    "    img_orig = surgeon([prompt], img_size=512, n_steps=30, n_imgs=1, \n",
    "                       show_alpha=False, generator=gen_orig, replace_with=None)[0][0]\n",
    "    \n",
    "    surgeon.params['lambda'] = force\n",
    "    surgeon.params['alpha_threshold'] = sens\n",
    "    surgeon.concepts_to_erase = [remove]\n",
    "    \n",
    "    # 2. Modified (Surgery)\n",
    "    gen_mod = torch.Generator(\"cpu\").manual_seed(int(seed))\n",
    "    img_mod = surgeon([prompt], img_size=512, n_steps=30, n_imgs=1, \n",
    "                      show_alpha=False, generator=gen_mod, replace_with=inject)[0][0]\n",
    "    \n",
    "    return img_orig, img_mod\n",
    "\n",
    "# --- 2. LOGIC FOR EACH TAB ---\n",
    "def manual_process(prompt, remove, inject, force, sens, seed):\n",
    "    return core_generation(prompt, remove, inject, force, sens, seed)\n",
    "\n",
    "def ml_process(prompt, remove, inject, seed):\n",
    "    if not models_ready: return None, None, \"Model Error\"\n",
    "    \n",
    "    # Predict\n",
    "    emb = get_clip_features(f\"{prompt} -> change to {inject}\").numpy()\n",
    "    pred = ml_model.predict(emb)[0]\n",
    "    f, s = float(pred[0]), float(pred[1])\n",
    "    \n",
    "    # Generate\n",
    "    img_o, img_m = core_generation(prompt, remove, inject, f, s, seed)\n",
    "    return img_o, img_m, f\"🤖 Random Forest suggests: Force={f:.2f}, Sens={s:.2f}\"\n",
    "\n",
    "def dl_process(prompt, remove, inject, seed):\n",
    "    if not models_ready: return None, None, \"Model Error\"\n",
    "    \n",
    "    # Predict\n",
    "    emb = get_clip_features(f\"{prompt} -> change to {inject}\") # Tensor\n",
    "    with torch.no_grad():\n",
    "        pred = dl_model(emb).numpy()[0]\n",
    "    f, s = float(pred[0]), float(pred[1])\n",
    "    \n",
    "    # Generate\n",
    "    img_o, img_m = core_generation(prompt, remove, inject, f, s, seed)\n",
    "    return img_o, img_m, f\"🧠 Deep Learning suggests: Force={f:.2f}, Sens={s:.2f}\"\n",
    "\n",
    "# --- 3. GRADIO INTERFACE ---\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# 🏥 Semantic Surgery: The Ultimate Comparison\", elem_id=\"title\")\n",
    "    gr.Markdown(\"Explore manual control vs. automated predictions using Machine Learning and Deep Learning.\")\n",
    "\n",
    "    with gr.Tabs():\n",
    "        \n",
    "        # --- TAB 1: MANUAL ---\n",
    "        with gr.TabItem(\"🎛️ Manual Control\"):\n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    m_prompt = gr.Textbox(label=\"Base Prompt\", value=\"a cat in a park\")\n",
    "                    m_remove = gr.Textbox(label=\"Remove\", value=\"cat\")\n",
    "                    m_inject = gr.Textbox(label=\"Inject\", value=\"bunny\")\n",
    "                    with gr.Group():\n",
    "                        gr.Markdown(\"**Surgical Parameters**\")\n",
    "                        m_force = gr.Slider(0.6, 1.5, value=1.0, label=\"Force\")\n",
    "                        m_sens = gr.Slider(0.05, 0.3, value=0.15, label=\"Sensitivity\")\n",
    "                    m_seed = gr.Number(value=42, label=\"Seed\")\n",
    "                    m_btn = gr.Button(\"Run Manual Surgery\", variant=\"primary\")\n",
    "                with gr.Column():\n",
    "                    with gr.Row():\n",
    "                        m_out1 = gr.Image(label=\"Original\")\n",
    "                        m_out2 = gr.Image(label=\"Modified\")\n",
    "            m_btn.click(manual_process, [m_prompt, m_remove, m_inject, m_force, m_sens, m_seed], [m_out1, m_out2])\n",
    "\n",
    "        # --- TAB 2: ML AUTOMATION ---\n",
    "        with gr.TabItem(\"🤖 ML Autopilot (Random Forest)\"):\n",
    "            gr.Markdown(\"Uses Classical ML to predict parameters based on CLIP embeddings.\")\n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    ml_prompt = gr.Textbox(label=\"Base Prompt\", value=\"a cat in a park\")\n",
    "                    ml_remove = gr.Textbox(label=\"Remove\", value=\"cat\")\n",
    "                    ml_inject = gr.Textbox(label=\"Inject\", value=\"bunny\")\n",
    "                    ml_seed = gr.Number(value=42, label=\"Seed\")\n",
    "                    ml_btn = gr.Button(\"Ask Random Forest\", variant=\"secondary\")\n",
    "                    ml_info = gr.Textbox(label=\"Model Output\", interactive=False)\n",
    "                with gr.Column():\n",
    "                    with gr.Row():\n",
    "                        ml_out1 = gr.Image(label=\"Original\")\n",
    "                        ml_out2 = gr.Image(label=\"Modified\")\n",
    "            ml_btn.click(ml_process, [ml_prompt, ml_remove, ml_inject, ml_seed], [ml_out1, ml_out2, ml_info])\n",
    "\n",
    "        # --- TAB 3: DL AUTOMATION ---\n",
    "        with gr.TabItem(\"🧠 DL Autopilot (Neural Net)\"):\n",
    "            gr.Markdown(\"Uses the **SurgeryNet (Deep Learning)** model trained for 600 epochs.\")\n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    dl_prompt = gr.Textbox(label=\"Base Prompt\", value=\"a cat in a park\")\n",
    "                    dl_remove = gr.Textbox(label=\"Remove\", value=\"cat\")\n",
    "                    dl_inject = gr.Textbox(label=\"Inject\", value=\"bunny\")\n",
    "                    dl_seed = gr.Number(value=42, label=\"Seed\")\n",
    "                    dl_btn = gr.Button(\"Ask Neural Network\", variant=\"primary\")\n",
    "                    dl_info = gr.Textbox(label=\"Model Output\", interactive=False)\n",
    "                with gr.Column():\n",
    "                    with gr.Row():\n",
    "                        dl_out1 = gr.Image(label=\"Original\")\n",
    "                        dl_out2 = gr.Image(label=\"Modified\")\n",
    "            dl_btn.click(dl_process, [dl_prompt, dl_remove, dl_inject, dl_seed], [dl_out1, dl_out2, dl_info])\n",
    "\n",
    "# Launch\n",
    "demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "surgery",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
